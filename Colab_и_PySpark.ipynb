{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Перевод [ноутбука](https://colab.research.google.com/drive/1G894WS7ltIUTusWWmsCnF_zQhQqZCDOc#scrollTo=K_q3Yzc9qYc0), все права принадлежат автору"
      ],
      "metadata": {
        "id": "jv2SDo3-t4MR"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iox_ufgbqDXa"
      },
      "source": [
        "<h1><center>Введение в Google Colab и PySpark</center></h1>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1qV6Grv7qIa9"
      },
      "source": [
        "## Table Of Contents:\n",
        "<ol>\n",
        "<li><a href=\"#objective\">Objective</a></li>\n",
        "<li><a href=\"#prerequisite\">Prerequisite</a></li>\n",
        "<li><a href=\"#notes-from-the-author\">Notes from the Author</a></li>\n",
        "<li><a href=\"#big-data-pyspark-and-colaboratory\">Big data, PySpark and Colaboratory</a>\n",
        "    <ol>\n",
        "        <li><a href=\"#big-data\">Big data</a></li>\n",
        "        <li><a href=\"#pyspark\">PySpark</a></li>\n",
        "        <li><a href=\"#colaboratory\">Colaboratory</a></li>\n",
        "    </ol>\n",
        "</li>\n",
        "<li><a href=\"#jupyter-notebook-basics\">Jupyter Notebook Basics</a>\n",
        "    <ol>\n",
        "        <li><a href=\"#code-cells\">Code cells</a></li>\n",
        "        <li><a href=\"#text-cells\">Text cells</a></li>\n",
        "        <li><a href=\"#access-to-the-shell\">Access to the shell</a></li>\n",
        "        <li><a href=\"#installing-spark\">Installing Spark</a></li>\n",
        "    </ol>\n",
        "</li>\n",
        "<li><a href=\"#exploring-the-dataset\">Exploring the Dataset</a>\n",
        "    <ol>\n",
        "        <li><a href=\"#loading-the-dataset\">Loading the Dataset</a></li>\n",
        "        <li><a href=\"#viewing-the-dataframe\">Viewing the Dataframe</a></li>\n",
        "        <li><a href=\"#viewing-dataframe-columns\">Viewing Dataframe Columns</a></li>\n",
        "        <li><a href=\"#dataframe-schema\">Dataframe Schema</a>\n",
        "          <ul>\n",
        "            <li><a href=\"#implicit-schema-inference\">Inferring Schema Implicitly</a></li>\n",
        "            <li><a href=\"#explicit-schema-inference\">Defining Schema Explicitly</a></li>\n",
        "          </ul>\n",
        "        </li>\n",
        "    </ol>\n",
        "</li>\n",
        "<li><a href=\"#dataframe-operations-on-columns\">DataFrame Operations on Columns</a>\n",
        "    <ol>\n",
        "        <li><a href=\"#selecting-columns\">Selecting Columns</a></li>\n",
        "        <li><a href=\"#selecting-multiple-columns\">Selecting Multiple Columns</a></li>\n",
        "        <li><a href=\"#adding-new-columns\">Adding New Columns</a></li>\n",
        "        <li><a href=\"#renaming-columns\">Renaming Columns</a>\n",
        "        <li><a href=\"#grouping-by-columns\">Grouping By Columns</a>\n",
        "        <li><a href=\"#removing-columns\">Removing Columns</a>\n",
        "    </ol>\n",
        "</li>\n",
        "<li><a href=\"#dataframe-operations-on-rows\">DataFrame Operations on Rows</a>\n",
        "    <ol>\n",
        "        <li><a href=\"#filtering-rows\">Filtering Rows</a></li>\n",
        "        <li><a href=\"#get-distinct-rows\">Get Distinct Rows</a></li>\n",
        "        <li><a href=\"#sorting-rows\">Sorting Rows</a></li>\n",
        "        <li><a href=\"#union-dataframes\">Union Dataframes</a>\n",
        "    </ol>\n",
        "</li>\n",
        "<li><a href=\"#common-data-manipulation-functions\">Common Data Manipulation Functions</a>\n",
        "    <ol>\n",
        "        <li><a href=\"#string-functions\">String Functions</a></li>\n",
        "        <li><a href=\"#numeric-functions\">Numeric Functions</a></li>\n",
        "        <li><a href=\"#operations-on-date\">Operations on Date</a></li>\n",
        "    </ol>\n",
        "</li>\n",
        "<li><a href=\"#joins-in-pyspark\">Joins in PySpark</a></li>\n",
        "<li><a href=\"#spark-sql\">Spark SQL</a></li>\n",
        "<li><a href=\"#rdd\">RDD</a></li>\n",
        "<li><a href=\"#user-defined-functions-udf\">User-Defined Functions (UDF)</a></li>\n",
        "<li><a href=\"#common-questions\">Common Questions</a>\n",
        "    <ol>\n",
        "        <li><a href=\"#recommended-ide\">Recommended IDE</a></li>\n",
        "        <li><a href=\"#submitting-a-spark-job\">Submitting a Spark Job</a></li>\n",
        "        <li><a href=\"#creating-dataframes\">Creating Dataframes</a></li>\n",
        "        <li><a href=\"#drop-duplicates\">Drop Duplicates</a></li>\n",
        "        <li><a href=\"#fine-tuning-a-pyspark-job\">Fine Tuning a PySpark Job</a>\n",
        "          <ul>\n",
        "            <li><a href=\"#emr-sizing\">EMR Sizing</a></li>\n",
        "            <li><a href=\"#spark-configurations\">Spark Configurations</a></li>\n",
        "            <li><a href=\"#job-tuning\">Job Tuning</a>\n",
        "            <li><a href=\"#best-practices\">Best Practices</a>\n",
        "          </ul>\n",
        "        </li>\n",
        "    </ol>\n",
        "</li>\n",
        "</ol>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FFnYZltvqLgt"
      },
      "source": [
        "<a id='objective'></a>\n",
        "## Цель\n",
        "Цель этого ноутбука:\n",
        "><li>Дать правильное представление о различных доступных функциях PySpark. </li>\n",
        "><li>Краткое введение в Google Colab, поскольку именно на этой платформе написан этот блокнот. </li>\n",
        "\n",
        "Изучив этот блокнот, вы сможете эффективно писать программы на pyspark. Идеальный способ использования - просмотреть приведенные примеры и затем опробовать их на Colab. В конце есть несколько практических вопросов, которые вы можете использовать для самопроверки."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YR1CO3FTqO5h"
      },
      "source": [
        "<a id='prerequisite'></a>\n",
        "## Предварительные сведения\n",
        "><li>Хотя в этом блокноте будут даны некоторые теоретические сведения о pyspark и больших данных, я рекомендую всем прочитать об этом подробнее и получить более глубокое представление о том, как выполняются функции и какова актуальность больших данных в текущем сценарии.\n",
        "><li>Хорошее понимание языка python будет дополнительным бонусом."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bGbIBPLHqVXc"
      },
      "source": [
        "<a id='notes-from-the-author'></a>\n",
        "## Заметки автора\n",
        "\n",
        "Это руководство было создано с помощью Google Colab, поэтому код, который вы видите здесь, предназначен для работы на ноутбуке colab.<br>\n",
        "В ней рассматриваются основные [Функции PySpark](https://spark.apache.org/docs/latest/api/python/index.html) и краткое введение о том, как использовать [Colab](https://colab.research.google.com/notebooks/basic_features_overview.ipynb). <br>\n",
        "Если вы хотите просмотреть мой блокнот для коллаба по данному уроку, вы можете посмотреть его [здесь](https://colab.research.google.com/drive/1G894WS7ltIUTusWWmsCnF_zQhQqZCDOc). Впечатления от просмотра и читабельность намного лучше. <br>\n",
        "Если вы хотите попробовать что-то с этим блокнотом в качестве основы, не стесняйтесь скачать его из моего репозитория [здесь](https://github.com/jacobceles/knowledge-repo/blob/master/pyspark/Colab%20and%20PySpark.ipynb) а затем использовать его в jupyter notebook."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K_q3Yzc9qYc0"
      },
      "source": [
        "<a id='big-data-pyspark-and-colaboratory'></a>\n",
        "## Большие данные, PySpark и Colaboratory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5gs9JXCWqb9s"
      },
      "source": [
        "<a id='big-data'></a>\n",
        "### Big data (Большие данные)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q3UkLT6yqebl"
      },
      "source": [
        "Под большими данными обычно понимаются данные такого огромного объема, что обычные решения для хранения данных не могут эффективно их хранить и обрабатывать. В нашу эпоху данные генерируются с абсурдной скоростью. Данные собираются о каждом движении человека. Основная масса больших данных поступает из трех основных источников:\n",
        "<ol>\n",
        "   <li>Социальные данные</li>\n",
        "   <li>Данные машины</li>\n",
        "   <li>Транзакционные данные</li>\n",
        "</ol>\n",
        "\n",
        "Среди распространенных примеров источников таких данных - интернет-поиск, сообщения в Facebook, камеры дверного звонка, смарт-часы, история покупок в Интернете и т. д. Каждое действие создает данные, вопрос лишь в том, есть ли способ их собрать или нет.  Но что интересно, из всех этих собранных данных даже 5 % не используются в полной мере. В отрасли существует огромный спрос на профессионалов в области больших данных. Несмотря на то, что количество выпускников со специализацией в области больших данных растет, проблема в том, что они не имеют практических знаний о сценариях работы с большими данными, что приводит к плохим архитектоникам и неэффективным методам обработки данных.\n",
        "\n",
        ">Если вам интересно узнать больше о ландшафте и применяемых технологиях, то вот [статья](https://hostingtribunal.com/blog/big-data-stats/) которая показалась мне очень интересной!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NhM3wLG2qhlN"
      },
      "source": [
        "<a id='pyspark'></a>\n",
        "### PySpark"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VBfC_kvjqjPj"
      },
      "source": [
        "Если вы работаете в области больших данных, то наверняка слышали о Spark. Если вы посмотрите на сайте [Apache Spark](https://spark.apache.org/), вы увидите, что он заявлен как `молниеносный унифицированный аналитический движок`. PySpark - это разновидность Spark, используемая для обработки и анализа огромных объемов данных. Если вы знакомы с python и пробовали использовать его для работы с огромными массивами данных, то должны знать, что время выполнения может быть просто смешным. Введите PySpark!\n",
        "\n",
        "Представьте, что ваши данные распределены по разным местам. Если вы попытаетесь доставить данные в одну точку и выполнить там свой код, это будет не только неэффективно, но и вызовет проблемы с памятью. Теперь давайте представим, что ваш код обращается к данным, а не данные приходят к вашему коду. Это поможет избежать лишних перемещений данных, что, соответственно, уменьшит время работы.\n",
        "\n",
        "PySpark - это Python API для Spark; это означает, что он может делать почти все то же самое, что и Python. Конвейеры машинного обучения (ML), разведочный анализ данных (в масштабе), ETL для платформы данных и многое другое! И все это в распределенном виде. Одна из лучших сторон pyspark заключается в том, что если вы уже знакомы с python, его очень легко освоить.\n",
        "\n",
        "Помимо PySpark, для обработки больших данных используется еще один язык - Scala. Scala часто в 10 раз быстрее, чем *Python*, поскольку он является родным для Hadoop, так как основан на JVM. Однако PySpark быстро набирает популярность из-за простоты использования, легкой кривой обучения и возможностей ML.\n",
        "\n",
        "Я кратко объясню, как работает задание PySpark, но я настоятельно рекомендую вам прочитать больше об [архитектуре](https://data-flair.training/blogs/how-apache-spark-works/) и как все работает. Прежде чем я перейду к делу, позвольте мне сперва рассказать о некоторых <u>базовых терминах</u>:\n",
        "\n",
        "<b>Кластер</b> это набор слабо или тесно связанных компьютеров, которые работают вместе так, что их можно рассматривать как единую систему.\n",
        "\n",
        "<b>Hadoop</b> - это масштабируемый и отказоустойчивый фреймворк с открытым исходным кодом, написанный на Java. Он эффективно обрабатывает большие объемы данных на кластере, состоящем из аппаратных средств. Hadoop - это не только система хранения, но и платформа для хранения и обработки больших данных.\n",
        "\n",
        "<b>HDFS</b> (распределенная файловая система Hadoop). Это одна из самых надежных в мире систем хранения данных. HDFS - это файловая система Hadoop, предназначенная для хранения очень больших файлов, работающая на кластере, состоящем из аппаратных средств.\n",
        "\n",
        "<b>MapReduce</b> - это фреймворк для обработки данных, который состоит из 2 этапов - Mapper и Reducer. Процедура map выполняет фильтрацию и сортировку, а метод reduce - суммарную операцию. Обычно он работает на кластере hadoop.\n",
        "\n",
        "<b>Трансформация</b> относится к операциям, применяемым к набору данных для создания нового набора данных. Примерами преобразований являются фильтр, groupBy и map.\n",
        "\n",
        "<b>Действия</b> Действия означают операцию, которая поручает Spark выполнить вычисления и отправить результат обратно драйверу. Это пример действия.\n",
        "\n",
        "Хорошо! Теперь, когда с этим покончено, позвольте мне объяснить, как выполняется задание spark. Проще говоря, каждый раз, когда вы отправляете задание pyspark, его код внутренне преобразуется в программу MapReduce и выполняется на виртуальной машине Java. Теперь одна из мыслей, которая может возникнуть у вас в голове, это: <br>`Так код преобразуется в программу MapReduce. Не означает ли это, что MapReduce быстрее, чем pySpark? Ответ на этот вопрос - большое НЕТ. Именно это делает работу с spark особенной. Spark способен обрабатывать огромное количество данных одновременно в своей распределенной среде. Для этого он использует обработку <u>в памяти</u>, что делает его почти в 100 раз быстрее, чем Hadoop. Еще один фактор, который делает его быстрым, - это <u>ленивая оценка</u>. Spark задерживает оценку настолько, насколько это возможно. Каждый раз, когда вы отправляете задание, Spark создает план действий по выполнению кода, а затем ничего не делает. Наконец, когда вы запрашиваете результат (т. е. вызываете действие), он выполняет план, который в основном состоит из всех преобразований, которые вы упомянули в своем коде. Вот, собственно, и вся суть.\n",
        "\n",
        "Напоследок я хочу рассказать еще об одной вещи. Spark состоит в основном из 4 модулей:\n",
        "\n",
        "<ol>\n",
        "    <li>Spark SQL - помогает писать программы spark с помощью SQL-подобных запросов.</li>\n",
        "    <li>Spark Streaming - расширение основного Spark API, обеспечивающее масштабируемую, высокопроизводительную, отказоустойчивую потоковую обработку живых потоков данных. Широко используется для обработки данных социальных сетей.</li>\n",
        "    <li>Spark MLLib - компонент машинного обучения в SPark. Он помогает обучать ML-модели на огромных массивах данных с очень высокой эффективностью. </li>\n",
        "    <li>Spark GraphX - компонент визуализации Spark. Он позволяет пользователям просматривать данные как в виде графиков, так и в виде коллекций без перемещения и дублирования данных.</li>\n",
        "</ol>\n",
        "\n",
        "Надеюсь, это изображение дает лучшее представление о том, о чем я говорю:\n",
        "<img alt=\"Spark Modules\" src=\"https://i.postimg.cc/cHj2Wxcq/spark-streaming-datanami.png\" />\n",
        "<center><font color='#666956'>Source: Datanami</font><center>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1NJMWs4NqnlF"
      },
      "source": [
        "<a id='colaboratory'></a>\n",
        "### Colaboratory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ynKFk6b7qoWr"
      },
      "source": [
        "По словам представителей Google: <br> `Colaboratory, или сокращенно Colab, - это продукт компании Google Research. Colab позволяет любому человеку писать и выполнять произвольный код на языке python через браузер и особенно хорошо подходит для машинного обучения, анализа данных и образования. С технической точки зрения, Colab - это размещенный сервис Jupyter notebook, который не требует настройки для использования и предоставляет бесплатный доступ к вычислительным ресурсам, включая GPU.`\n",
        "\n",
        "Причина, по которой я использовал colab, заключается в возможности совместного использования и бесплатных GPU и TPU. Да, вы правильно прочитали, БЕСПЛАТНЫЕ GPU И TPU! Для использования TPU ваша программа должна быть оптимизирована под него. Кроме того, он помогает удобно использовать различные сервисы Google. Он сохраняет данные в Google Drive, а все сервисы очень тесно связаны между собой.Я рекомендую вам изучить официальную [обзорную документацию] (https://colab.research.google.com/notebooks/basic_features_overview.ipynb) если вы хотите узнать об этом больше.\n",
        "Если у вас есть дополнительные вопросы о colab, пожалуйста, [обратитесь к этой ссылке](https://research.google.com/colaboratory/faq.html).\n",
        "\n",
        ">При использовании colab notebook вам потребуется активное интернет-соединение, чтобы поддерживать сессию. Если вы потеряете соединение, вам придется заново загружать наборы данных (датасеты)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_N5-lspH_N8B"
      },
      "source": [
        "<a id='jupyter-notebook-basics'></a>\n",
        "## Jupyter notebook basics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Ul54hAYyHyd"
      },
      "source": [
        "<a id='code-cells'></a>\n",
        "### Ячейки кода"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j38beRUTCI5c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "56223211-da18-47e3-ff57-f5b682d3bdeb"
      },
      "source": [
        "2*3"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "6"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Jewe_e9CIYa"
      },
      "source": [
        "from collections import Counter"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g8Y7w6_CCIIT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d6bc5b1e-c514-432d-af86-1e7ff139f5c7"
      },
      "source": [
        "print(\"This is a tutorial!\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This is a tutorial!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VOqLNkRKyUIS"
      },
      "source": [
        "<a id='text-cells'></a>\n",
        "### Ячейки текста"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IfbaUe-oq7DK"
      },
      "source": [
        "Hello world!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X6zdrH15_CCW"
      },
      "source": [
        "<a id='access-to-the-shell'></a>\n",
        "### Доступ к командной строке"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zdO9sjSdEVnr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c41bf2dd-0e7a-4de8-e381-adbda0bd5386"
      },
      "source": [
        "ls"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[0m\u001b[01;34msample_data\u001b[0m/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QF9e3lDDEX3I",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "a7842425-139f-46cf-e907-6b4a194337f9"
      },
      "source": [
        "pwd"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dd6t0uFzuR4X"
      },
      "source": [
        "<a id='installing-spark'></a>\n",
        "### Установка Spark"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6apGVff5h4ca"
      },
      "source": [
        "Установка Зависимостей:\n",
        "\n",
        "\n",
        "1.   Java 8\n",
        "2.   Apache Spark with hadoop и\n",
        "3.   Findspark (используется для определения местонахождения spark в системе)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tt7ZS1_wGgjn"
      },
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q http://archive.apache.org/dist/spark/spark-3.1.1/spark-3.1.1-bin-hadoop3.2.tgz\n",
        "!tar xf spark-3.1.1-bin-hadoop3.2.tgz\n",
        "!pip install -q findspark"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C3x0ZRLxjMVr"
      },
      "source": [
        "Установка переменных окружения:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sdOOq4twHN1K"
      },
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.1.1-bin-hadoop3.2\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ACYMwhgHTYz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8a9f148d-6b0c-48f2-eeeb-51fcdfd2f060"
      },
      "source": [
        "!ls"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sample_data  spark-3.1.1-bin-hadoop3.2\tspark-3.1.1-bin-hadoop3.2.tgz\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KR1zLBk1998Z",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "d60a83dc-a235-4366-d9a7-2a2ebe4b9129"
      },
      "source": [
        "import findspark\n",
        "findspark.init()\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n",
        "spark.conf.set(\"spark.sql.repl.eagerEval.enabled\", True) # Property used to format output tables better\n",
        "spark"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x79d5e6bf8ee0>"
            ],
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://eac9a252cf85:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.1.1</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>pyspark-shell</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hmIqq6xPK7m7"
      },
      "source": [
        "<a id='exploring-the-dataset'></a>\n",
        "## Изучение набора данных (датасета)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VZwsr57lwPgq"
      },
      "source": [
        "<a id='loading-the-dataset'></a>\n",
        "### Загрузка набора данных"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hQ3zmGACLKlN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "860411a3-aee3-4024-f116-0602e58f7d1d"
      },
      "source": [
        "# Загрузка и предварительная обработка данных автомобилей Данные загружены с сайта https://perso.telecom-paristech.fr/eagan/class/igr204/datasets\n",
        "!wget https://jacobceles.github.io/knowledge_repo/colab_and_pyspark/cars.csv"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-01-26 13:56:17--  https://jacobceles.github.io/knowledge_repo/colab_and_pyspark/cars.csv\n",
            "Resolving jacobceles.github.io (jacobceles.github.io)... 185.199.108.153, 185.199.109.153, 185.199.110.153, ...\n",
            "Connecting to jacobceles.github.io (jacobceles.github.io)|185.199.108.153|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: https://jacobcelestine.com/knowledge_repo/colab_and_pyspark/cars.csv [following]\n",
            "--2024-01-26 13:56:17--  https://jacobcelestine.com/knowledge_repo/colab_and_pyspark/cars.csv\n",
            "Resolving jacobcelestine.com (jacobcelestine.com)... 185.199.108.153, 185.199.109.153, 185.199.110.153, ...\n",
            "Connecting to jacobcelestine.com (jacobcelestine.com)|185.199.108.153|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 22608 (22K) [text/csv]\n",
            "Saving to: ‘cars.csv’\n",
            "\n",
            "\rcars.csv              0%[                    ]       0  --.-KB/s               \rcars.csv            100%[===================>]  22.08K  --.-KB/s    in 0s      \n",
            "\n",
            "2024-01-26 13:56:18 (63.7 MB/s) - ‘cars.csv’ saved [22608/22608]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wpq2jYvIMOJy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3a3bfd10-eb00-45cb-c565-5430f1c1f235"
      },
      "source": [
        "!ls"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cars.csv  sample_data  spark-3.1.1-bin-hadoop3.2  spark-3.1.1-bin-hadoop3.2.tgz\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hz6ALr5mMqZt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ae73b919-f68a-4158-9edf-a5dfd7a7fa85"
      },
      "source": [
        "# Загрузите данные из csv во фрейм данных.\n",
        "# header=True означает, что первая строка - это заголовок\n",
        "# sep=';' означает, что столбцы разделены с помощью ''.\n",
        "df = spark.read.csv('cars.csv', header=True, sep=\";\")\n",
        "df.show(5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+----+---------+------------+----------+------+------------+-----+------+\n",
            "|                 Car| MPG|Cylinders|Displacement|Horsepower|Weight|Acceleration|Model|Origin|\n",
            "+--------------------+----+---------+------------+----------+------+------------+-----+------+\n",
            "|Chevrolet Chevell...|18.0|        8|       307.0|     130.0| 3504.|        12.0|   70|    US|\n",
            "|   Buick Skylark 320|15.0|        8|       350.0|     165.0| 3693.|        11.5|   70|    US|\n",
            "|  Plymouth Satellite|18.0|        8|       318.0|     150.0| 3436.|        11.0|   70|    US|\n",
            "|       AMC Rebel SST|16.0|        8|       304.0|     150.0| 3433.|        12.0|   70|    US|\n",
            "|         Ford Torino|17.0|        8|       302.0|     140.0| 3449.|        10.5|   70|    US|\n",
            "+--------------------+----+---------+------------+----------+------+------------+-----+------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N0lCS2LNwnoy"
      },
      "source": [
        "Приведенная выше команда загружает наши данные в фрейм данных (DF). Фрейм данных - это двумерная структура данных с метками, содержащая столбцы потенциально разных типов."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_QwZtWxZRCBn"
      },
      "source": [
        "<a id='viewing-the-dataframe'></a>\n",
        "### Просмотр кадра данных"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "50LZ3S8_PMg_"
      },
      "source": [
        "В PySpark есть несколько способов просмотреть кадр данных (DF):\n",
        "\n",
        "1.   `df.take(5)` вернет список из пяти объектов Row.\n",
        "2.   `df.collect()` получит все данные из всего DataFrame. Будьте очень осторожны при его использовании, потому что если у вас большой набор данных, вы можете легко вывести из строя узел драйвера.\n",
        "3.   `df.show()` - это наиболее часто используемый метод для просмотра фрейма данных. Есть несколько параметров, которые можно передать этому методу, например количество строк и усечение. Например, `df.show(5, False)` или `df.show(5, truncate=False)` покажет все данные без какого-либо усечения.\n",
        "4.   `df.limit(5)` **возвратит новый DataFrame**, взяв первые n строк. Поскольку искра по своей природе является распределенной, нет никакой гарантии, что `df.limit()` будет давать каждый раз одинаковые результаты.\n",
        "\n",
        "Ниже мы рассмотрим некоторые из них в действии:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I1qqkqcfxM0v",
        "outputId": "2943a97b-dc9a-407c-8a74-0b36ccd5c039"
      },
      "source": [
        "df.show(5, truncate=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------------------+----+---------+------------+----------+------+------------+-----+------+\n",
            "|Car                      |MPG |Cylinders|Displacement|Horsepower|Weight|Acceleration|Model|Origin|\n",
            "+-------------------------+----+---------+------------+----------+------+------------+-----+------+\n",
            "|Chevrolet Chevelle Malibu|18.0|8        |307.0       |130.0     |3504. |12.0        |70   |US    |\n",
            "|Buick Skylark 320        |15.0|8        |350.0       |165.0     |3693. |11.5        |70   |US    |\n",
            "|Plymouth Satellite       |18.0|8        |318.0       |150.0     |3436. |11.0        |70   |US    |\n",
            "|AMC Rebel SST            |16.0|8        |304.0       |150.0     |3433. |12.0        |70   |US    |\n",
            "|Ford Torino              |17.0|8        |302.0       |140.0     |3449. |10.5        |70   |US    |\n",
            "+-------------------------+----+---------+------------+----------+------+------------+-----+------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 161
        },
        "id": "R9zwzswIxXF9",
        "outputId": "8ec6c739-812e-45a9-a42d-35278ac6c343"
      },
      "source": [
        "df.limit(5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "+--------------------+----+---------+------------+----------+------+------------+-----+------+\n",
              "|                 Car| MPG|Cylinders|Displacement|Horsepower|Weight|Acceleration|Model|Origin|\n",
              "+--------------------+----+---------+------------+----------+------+------------+-----+------+\n",
              "|Chevrolet Chevell...|18.0|        8|       307.0|     130.0| 3504.|        12.0|   70|    US|\n",
              "|   Buick Skylark 320|15.0|        8|       350.0|     165.0| 3693.|        11.5|   70|    US|\n",
              "|  Plymouth Satellite|18.0|        8|       318.0|     150.0| 3436.|        11.0|   70|    US|\n",
              "|       AMC Rebel SST|16.0|        8|       304.0|     150.0| 3433.|        12.0|   70|    US|\n",
              "|         Ford Torino|17.0|        8|       302.0|     140.0| 3449.|        10.5|   70|    US|\n",
              "+--------------------+----+---------+------------+----------+------+------------+-----+------+"
            ],
            "text/html": [
              "<table border='1'>\n",
              "<tr><th>Car</th><th>MPG</th><th>Cylinders</th><th>Displacement</th><th>Horsepower</th><th>Weight</th><th>Acceleration</th><th>Model</th><th>Origin</th></tr>\n",
              "<tr><td>Chevrolet Chevell...</td><td>18.0</td><td>8</td><td>307.0</td><td>130.0</td><td>3504.</td><td>12.0</td><td>70</td><td>US</td></tr>\n",
              "<tr><td>Buick Skylark 320</td><td>15.0</td><td>8</td><td>350.0</td><td>165.0</td><td>3693.</td><td>11.5</td><td>70</td><td>US</td></tr>\n",
              "<tr><td>Plymouth Satellite</td><td>18.0</td><td>8</td><td>318.0</td><td>150.0</td><td>3436.</td><td>11.0</td><td>70</td><td>US</td></tr>\n",
              "<tr><td>AMC Rebel SST</td><td>16.0</td><td>8</td><td>304.0</td><td>150.0</td><td>3433.</td><td>12.0</td><td>70</td><td>US</td></tr>\n",
              "<tr><td>Ford Torino</td><td>17.0</td><td>8</td><td>302.0</td><td>140.0</td><td>3449.</td><td>10.5</td><td>70</td><td>US</td></tr>\n",
              "</table>\n"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eUazdCEmu_sp"
      },
      "source": [
        "<a id='viewing-dataframe-columns'></a>\n",
        "### Просмотр столбцов фрейма данных"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9o7jsazcu-13",
        "outputId": "54877c65-62ce-4eae-f1ea-b948376eeae2"
      },
      "source": [
        "df.columns"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Car',\n",
              " 'MPG',\n",
              " 'Cylinders',\n",
              " 'Displacement',\n",
              " 'Horsepower',\n",
              " 'Weight',\n",
              " 'Acceleration',\n",
              " 'Model',\n",
              " 'Origin']"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3lfS2DhHuhPl"
      },
      "source": [
        "<a id='dataframe-schema'></a>\n",
        "### Схема фрейма данных"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-xX7hRoW_cXY"
      },
      "source": [
        "Для просмотра типов данных в фрейме обычно используются два метода:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w6qwTjGsNxrw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "69006dfc-3583-4196-8421-3896a40512fe"
      },
      "source": [
        "df.dtypes"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Car', 'string'),\n",
              " ('MPG', 'string'),\n",
              " ('Cylinders', 'string'),\n",
              " ('Displacement', 'string'),\n",
              " ('Horsepower', 'string'),\n",
              " ('Weight', 'string'),\n",
              " ('Acceleration', 'string'),\n",
              " ('Model', 'string'),\n",
              " ('Origin', 'string')]"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CCGTFlCWRPw4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c66d02f2-28f6-4af4-bff8-2cd86c5124a6"
      },
      "source": [
        "df.printSchema()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- Car: string (nullable = true)\n",
            " |-- MPG: string (nullable = true)\n",
            " |-- Cylinders: string (nullable = true)\n",
            " |-- Displacement: string (nullable = true)\n",
            " |-- Horsepower: string (nullable = true)\n",
            " |-- Weight: string (nullable = true)\n",
            " |-- Acceleration: string (nullable = true)\n",
            " |-- Model: string (nullable = true)\n",
            " |-- Origin: string (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RXx5ATpZ9oor"
      },
      "source": [
        "<a id='implicit-schema-inference'></a>\n",
        "#### Неявный вывод схемы"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7TeflTUp8l29"
      },
      "source": [
        "Мы можем использовать параметр `inferschema=true`, чтобы автоматически подставлять входную схему при загрузке данных. Пример показан ниже:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qym5MjCi894N",
        "outputId": "c2657dcc-67f4-4f13-b142-896c6a5d176b"
      },
      "source": [
        "df = spark.read.csv('cars.csv', header=True, sep=\";\", inferSchema=True)\n",
        "df.printSchema()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- Car: string (nullable = true)\n",
            " |-- MPG: double (nullable = true)\n",
            " |-- Cylinders: integer (nullable = true)\n",
            " |-- Displacement: double (nullable = true)\n",
            " |-- Horsepower: double (nullable = true)\n",
            " |-- Weight: decimal(4,0) (nullable = true)\n",
            " |-- Acceleration: double (nullable = true)\n",
            " |-- Model: integer (nullable = true)\n",
            " |-- Origin: string (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G6jTedYd-Dhb"
      },
      "source": [
        "Как видите, тип данных был выведен автоматически spark с правильной точностью для десятичного типа. Проблема, которая может здесь возникнуть, заключается в том, что иногда, когда вам приходится читать несколько файлов с разными схемами в разных файлах, может возникнуть проблема с неявным инферированием, приводящая к нулевым значениям в некоторых столбцах. Поэтому давайте также посмотрим, как определить схемы явно."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yTVjYqeRuxWn"
      },
      "source": [
        "<a id='explicit-schema-inference'></a>\n",
        "#### Определение схемы в явном виде"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xpsaQ4JMRUiS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cd095157-2aa6-4748-b160-fbfb87fe3429"
      },
      "source": [
        "from pyspark.sql.types import *\n",
        "df.columns"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Car',\n",
              " 'MPG',\n",
              " 'Cylinders',\n",
              " 'Displacement',\n",
              " 'Horsepower',\n",
              " 'Weight',\n",
              " 'Acceleration',\n",
              " 'Model',\n",
              " 'Origin']"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ik62VX34SlFh"
      },
      "source": [
        "# Создание списка схем в формате column_name, data_type\n",
        "labels = [\n",
        "     ('Car',StringType()),\n",
        "     ('MPG',DoubleType()),\n",
        "     ('Cylinders',IntegerType()),\n",
        "     ('Displacement',DoubleType()),\n",
        "     ('Horsepower',DoubleType()),\n",
        "     ('Weight',DoubleType()),\n",
        "     ('Acceleration',DoubleType()),\n",
        "     ('Model',IntegerType()),\n",
        "     ('Origin',StringType())\n",
        "]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T-Fp5y_oU9SF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2b161677-cfed-4d7a-e71c-1bd626d29a07"
      },
      "source": [
        "# Создание схемы, которая будет передаваться при чтении csv\n",
        "schema = StructType([StructField (x[0], x[1], True) for x in labels])\n",
        "schema"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "StructType(List(StructField(Car,StringType,true),StructField(MPG,DoubleType,true),StructField(Cylinders,IntegerType,true),StructField(Displacement,DoubleType,true),StructField(Horsepower,DoubleType,true),StructField(Weight,DoubleType,true),StructField(Acceleration,DoubleType,true),StructField(Model,IntegerType,true),StructField(Origin,StringType,true)))"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sgC7gtL5VTls",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "be34147b-095b-4199-9a02-ed7eeeba3c00"
      },
      "source": [
        "df = spark.read.csv('cars.csv', header=True, sep=\";\", schema=schema)\n",
        "df.printSchema()\n",
        "# Схема приходит по мере того, как мы отдаем!"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- Car: string (nullable = true)\n",
            " |-- MPG: double (nullable = true)\n",
            " |-- Cylinders: integer (nullable = true)\n",
            " |-- Displacement: double (nullable = true)\n",
            " |-- Horsepower: double (nullable = true)\n",
            " |-- Weight: double (nullable = true)\n",
            " |-- Acceleration: double (nullable = true)\n",
            " |-- Model: integer (nullable = true)\n",
            " |-- Origin: string (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dn2EAhesVmx0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "41841014-c431-44cc-ce2d-9bf0722326f3"
      },
      "source": [
        "df.show(truncate=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------------------+----+---------+------------+----------+------+------------+-----+------+\n",
            "|Car                             |MPG |Cylinders|Displacement|Horsepower|Weight|Acceleration|Model|Origin|\n",
            "+--------------------------------+----+---------+------------+----------+------+------------+-----+------+\n",
            "|Chevrolet Chevelle Malibu       |18.0|8        |307.0       |130.0     |3504.0|12.0        |70   |US    |\n",
            "|Buick Skylark 320               |15.0|8        |350.0       |165.0     |3693.0|11.5        |70   |US    |\n",
            "|Plymouth Satellite              |18.0|8        |318.0       |150.0     |3436.0|11.0        |70   |US    |\n",
            "|AMC Rebel SST                   |16.0|8        |304.0       |150.0     |3433.0|12.0        |70   |US    |\n",
            "|Ford Torino                     |17.0|8        |302.0       |140.0     |3449.0|10.5        |70   |US    |\n",
            "|Ford Galaxie 500                |15.0|8        |429.0       |198.0     |4341.0|10.0        |70   |US    |\n",
            "|Chevrolet Impala                |14.0|8        |454.0       |220.0     |4354.0|9.0         |70   |US    |\n",
            "|Plymouth Fury iii               |14.0|8        |440.0       |215.0     |4312.0|8.5         |70   |US    |\n",
            "|Pontiac Catalina                |14.0|8        |455.0       |225.0     |4425.0|10.0        |70   |US    |\n",
            "|AMC Ambassador DPL              |15.0|8        |390.0       |190.0     |3850.0|8.5         |70   |US    |\n",
            "|Citroen DS-21 Pallas            |0.0 |4        |133.0       |115.0     |3090.0|17.5        |70   |Europe|\n",
            "|Chevrolet Chevelle Concours (sw)|0.0 |8        |350.0       |165.0     |4142.0|11.5        |70   |US    |\n",
            "|Ford Torino (sw)                |0.0 |8        |351.0       |153.0     |4034.0|11.0        |70   |US    |\n",
            "|Plymouth Satellite (sw)         |0.0 |8        |383.0       |175.0     |4166.0|10.5        |70   |US    |\n",
            "|AMC Rebel SST (sw)              |0.0 |8        |360.0       |175.0     |3850.0|11.0        |70   |US    |\n",
            "|Dodge Challenger SE             |15.0|8        |383.0       |170.0     |3563.0|10.0        |70   |US    |\n",
            "|Plymouth 'Cuda 340              |14.0|8        |340.0       |160.0     |3609.0|8.0         |70   |US    |\n",
            "|Ford Mustang Boss 302           |0.0 |8        |302.0       |140.0     |3353.0|8.0         |70   |US    |\n",
            "|Chevrolet Monte Carlo           |15.0|8        |400.0       |150.0     |3761.0|9.5         |70   |US    |\n",
            "|Buick Estate Wagon (sw)         |14.0|8        |455.0       |225.0     |3086.0|10.0        |70   |US    |\n",
            "+--------------------------------+----+---------+------------+----------+------+------------+-----+------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MDCO3TEe95OY"
      },
      "source": [
        "Как мы видим, данные были успешно загружены с указанными типами данных."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rsD48rckdHPe"
      },
      "source": [
        "<a id='dataframe-operations-on-columns'></a>\n",
        "## Операции над столбцами фрейма данных"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cMlxdWfSY8ks"
      },
      "source": [
        "В этом разделе мы рассмотрим следующее:\n",
        "\n",
        "1.   Выбор столбцов\n",
        "2.   Выбор нескольких столбцов\n",
        "3.   Добавление новых столбцов\n",
        "4.   Переименование столбцов\n",
        "5.   Группировка по столбцам\n",
        "6.   Удаление столбцов\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ikGR5pDICTu7"
      },
      "source": [
        "<a id='selecting-columns'></a>\n",
        "### Выбор столбцов"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-VMwIwi2rj_o"
      },
      "source": [
        "В PySpark существует несколько способов сделать выборку. Ниже вы можете узнать, чем они отличаются друг от друга:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ge9-_ygideWk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0462c34f-39a8-42e1-ac1d-1cf31fbdf35c"
      },
      "source": [
        "# 1st method\n",
        "# Column name is case sensitive in this usage\n",
        "print(df.Car)\n",
        "print(\"*\"*20)\n",
        "df.select(df.Car).show(truncate=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Column<'Car'>\n",
            "********************\n",
            "+--------------------------------+\n",
            "|Car                             |\n",
            "+--------------------------------+\n",
            "|Chevrolet Chevelle Malibu       |\n",
            "|Buick Skylark 320               |\n",
            "|Plymouth Satellite              |\n",
            "|AMC Rebel SST                   |\n",
            "|Ford Torino                     |\n",
            "|Ford Galaxie 500                |\n",
            "|Chevrolet Impala                |\n",
            "|Plymouth Fury iii               |\n",
            "|Pontiac Catalina                |\n",
            "|AMC Ambassador DPL              |\n",
            "|Citroen DS-21 Pallas            |\n",
            "|Chevrolet Chevelle Concours (sw)|\n",
            "|Ford Torino (sw)                |\n",
            "|Plymouth Satellite (sw)         |\n",
            "|AMC Rebel SST (sw)              |\n",
            "|Dodge Challenger SE             |\n",
            "|Plymouth 'Cuda 340              |\n",
            "|Ford Mustang Boss 302           |\n",
            "|Chevrolet Monte Carlo           |\n",
            "|Buick Estate Wagon (sw)         |\n",
            "+--------------------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YxP1su8veNde"
      },
      "source": [
        "**ПРИМЕЧАНИЕ:**\n",
        "\n",
        "> **Мы не можем всегда использовать точечную нотацию, потому что это приведет к сбою, если имена столбцов будут иметь зарезервированные имена или атрибуты класса фрейма данных. Кроме того, имена столбцов чувствительны к регистру, поэтому перед их использованием необходимо убедиться, что имена столбцов изменены на конкретный регистр.**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "md5zaET8dsr4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7b570ad1-cdff-4f64-9406-b66bc90660b4"
      },
      "source": [
        "# 2-й метод\n",
        "# Имя столбца здесь не зависит от регистра.\n",
        "print(df['car'])\n",
        "print(\"*\"*20)\n",
        "df.select(df['car']).show(truncate=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Column<'car'>\n",
            "********************\n",
            "+--------------------------------+\n",
            "|car                             |\n",
            "+--------------------------------+\n",
            "|Chevrolet Chevelle Malibu       |\n",
            "|Buick Skylark 320               |\n",
            "|Plymouth Satellite              |\n",
            "|AMC Rebel SST                   |\n",
            "|Ford Torino                     |\n",
            "|Ford Galaxie 500                |\n",
            "|Chevrolet Impala                |\n",
            "|Plymouth Fury iii               |\n",
            "|Pontiac Catalina                |\n",
            "|AMC Ambassador DPL              |\n",
            "|Citroen DS-21 Pallas            |\n",
            "|Chevrolet Chevelle Concours (sw)|\n",
            "|Ford Torino (sw)                |\n",
            "|Plymouth Satellite (sw)         |\n",
            "|AMC Rebel SST (sw)              |\n",
            "|Dodge Challenger SE             |\n",
            "|Plymouth 'Cuda 340              |\n",
            "|Ford Mustang Boss 302           |\n",
            "|Chevrolet Monte Carlo           |\n",
            "|Buick Estate Wagon (sw)         |\n",
            "+--------------------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Gkf14sHec9a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dfec1aee-fa15-415c-8c5f-5bc94b82ea9d"
      },
      "source": [
        "# 3-й способ\n",
        "# Имя столбца здесь не зависит от регистра\n",
        "from pyspark.sql.functions import col\n",
        "df.select(col('car')).show(truncate=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------------------+\n",
            "|car                             |\n",
            "+--------------------------------+\n",
            "|Chevrolet Chevelle Malibu       |\n",
            "|Buick Skylark 320               |\n",
            "|Plymouth Satellite              |\n",
            "|AMC Rebel SST                   |\n",
            "|Ford Torino                     |\n",
            "|Ford Galaxie 500                |\n",
            "|Chevrolet Impala                |\n",
            "|Plymouth Fury iii               |\n",
            "|Pontiac Catalina                |\n",
            "|AMC Ambassador DPL              |\n",
            "|Citroen DS-21 Pallas            |\n",
            "|Chevrolet Chevelle Concours (sw)|\n",
            "|Ford Torino (sw)                |\n",
            "|Plymouth Satellite (sw)         |\n",
            "|AMC Rebel SST (sw)              |\n",
            "|Dodge Challenger SE             |\n",
            "|Plymouth 'Cuda 340              |\n",
            "|Ford Mustang Boss 302           |\n",
            "|Chevrolet Monte Carlo           |\n",
            "|Buick Estate Wagon (sw)         |\n",
            "+--------------------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z6QsMfnNt3qF"
      },
      "source": [
        "<a id='selecting-multiple-columns'></a>\n",
        "### Выбор нескольких столбцов"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bPjLMhZ6uAQR",
        "outputId": "874e3608-3309-4bbc-eb67-0b79f86631a6"
      },
      "source": [
        "# 1-й способ\n",
        "# Имя столбца в данном случае чувствительно к регистру\n",
        "print(df.Car, df.Cylinders)\n",
        "print(\"*\"*40)\n",
        "df.select(df.Car, df.Cylinders).show(truncate=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Column<'Car'> Column<'Cylinders'>\n",
            "****************************************\n",
            "+--------------------------------+---------+\n",
            "|Car                             |Cylinders|\n",
            "+--------------------------------+---------+\n",
            "|Chevrolet Chevelle Malibu       |8        |\n",
            "|Buick Skylark 320               |8        |\n",
            "|Plymouth Satellite              |8        |\n",
            "|AMC Rebel SST                   |8        |\n",
            "|Ford Torino                     |8        |\n",
            "|Ford Galaxie 500                |8        |\n",
            "|Chevrolet Impala                |8        |\n",
            "|Plymouth Fury iii               |8        |\n",
            "|Pontiac Catalina                |8        |\n",
            "|AMC Ambassador DPL              |8        |\n",
            "|Citroen DS-21 Pallas            |4        |\n",
            "|Chevrolet Chevelle Concours (sw)|8        |\n",
            "|Ford Torino (sw)                |8        |\n",
            "|Plymouth Satellite (sw)         |8        |\n",
            "|AMC Rebel SST (sw)              |8        |\n",
            "|Dodge Challenger SE             |8        |\n",
            "|Plymouth 'Cuda 340              |8        |\n",
            "|Ford Mustang Boss 302           |8        |\n",
            "|Chevrolet Monte Carlo           |8        |\n",
            "|Buick Estate Wagon (sw)         |8        |\n",
            "+--------------------------------+---------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DMRMUrv7uHWa",
        "outputId": "b8fc1d07-d242-4492-dc19-8f3f65c94d47"
      },
      "source": [
        "# 2-й способ\n",
        "# Имя столбца не зависит от регистра при таком использовании\n",
        "print(df['car'],df['cylinders'])\n",
        "print(\"*\"*40)\n",
        "df.select(df['car'],df['cylinders']).show(truncate=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Column<'car'> Column<'cylinders'>\n",
            "****************************************\n",
            "+--------------------------------+---------+\n",
            "|car                             |cylinders|\n",
            "+--------------------------------+---------+\n",
            "|Chevrolet Chevelle Malibu       |8        |\n",
            "|Buick Skylark 320               |8        |\n",
            "|Plymouth Satellite              |8        |\n",
            "|AMC Rebel SST                   |8        |\n",
            "|Ford Torino                     |8        |\n",
            "|Ford Galaxie 500                |8        |\n",
            "|Chevrolet Impala                |8        |\n",
            "|Plymouth Fury iii               |8        |\n",
            "|Pontiac Catalina                |8        |\n",
            "|AMC Ambassador DPL              |8        |\n",
            "|Citroen DS-21 Pallas            |4        |\n",
            "|Chevrolet Chevelle Concours (sw)|8        |\n",
            "|Ford Torino (sw)                |8        |\n",
            "|Plymouth Satellite (sw)         |8        |\n",
            "|AMC Rebel SST (sw)              |8        |\n",
            "|Dodge Challenger SE             |8        |\n",
            "|Plymouth 'Cuda 340              |8        |\n",
            "|Ford Mustang Boss 302           |8        |\n",
            "|Chevrolet Monte Carlo           |8        |\n",
            "|Buick Estate Wagon (sw)         |8        |\n",
            "+--------------------------------+---------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RgQ20-4GugjR",
        "outputId": "ab82a55b-331d-416e-d214-461bd02ef2ee"
      },
      "source": [
        "# 3-й метод\n",
        "# Имя столбца не чувствительно к регистру при таком использовании\n",
        "from pyspark.sql.functions import col\n",
        "df.select(col('car'),col('cylinders')).show(truncate=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------------------+---------+\n",
            "|car                             |cylinders|\n",
            "+--------------------------------+---------+\n",
            "|Chevrolet Chevelle Malibu       |8        |\n",
            "|Buick Skylark 320               |8        |\n",
            "|Plymouth Satellite              |8        |\n",
            "|AMC Rebel SST                   |8        |\n",
            "|Ford Torino                     |8        |\n",
            "|Ford Galaxie 500                |8        |\n",
            "|Chevrolet Impala                |8        |\n",
            "|Plymouth Fury iii               |8        |\n",
            "|Pontiac Catalina                |8        |\n",
            "|AMC Ambassador DPL              |8        |\n",
            "|Citroen DS-21 Pallas            |4        |\n",
            "|Chevrolet Chevelle Concours (sw)|8        |\n",
            "|Ford Torino (sw)                |8        |\n",
            "|Plymouth Satellite (sw)         |8        |\n",
            "|AMC Rebel SST (sw)              |8        |\n",
            "|Dodge Challenger SE             |8        |\n",
            "|Plymouth 'Cuda 340              |8        |\n",
            "|Ford Mustang Boss 302           |8        |\n",
            "|Chevrolet Monte Carlo           |8        |\n",
            "|Buick Estate Wagon (sw)         |8        |\n",
            "+--------------------------------+---------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "85Lv3zSXCcOY"
      },
      "source": [
        "<a id='adding-new-columns'></a>\n",
        "### Добавление новых столбцов"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d_Y7dcAHu-Uz"
      },
      "source": [
        "Здесь мы рассмотрим три случая:\n",
        "\n",
        "1.   Добавление нового столбца\n",
        "2.   Добавление нескольких столбцов\n",
        "3.   Выведение нового столбца из существующего"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oFHUmRKZeCEV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5c56219d-4edd-42b1-85d6-77653052cb9d"
      },
      "source": [
        "# ПРИМЕР 1: Добавление нового столбца\n",
        "# Мы добавим новый столбец под названием 'first_column' в конце\n",
        "from pyspark.sql.functions import lit\n",
        "df = df.withColumn('first_column',lit(1))\n",
        "# lit означает литерал. Он заполняет строку заданным буквальным значением.\n",
        "# При добавлении статических данных / постоянных значений, это хорошая практика, чтобы использовать его.\n",
        "df.show(5,truncate=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------------------+----+---------+------------+----------+------+------------+-----+------+------------+\n",
            "|Car                      |MPG |Cylinders|Displacement|Horsepower|Weight|Acceleration|Model|Origin|first_column|\n",
            "+-------------------------+----+---------+------------+----------+------+------------+-----+------+------------+\n",
            "|Chevrolet Chevelle Malibu|18.0|8        |307.0       |130.0     |3504.0|12.0        |70   |US    |1           |\n",
            "|Buick Skylark 320        |15.0|8        |350.0       |165.0     |3693.0|11.5        |70   |US    |1           |\n",
            "|Plymouth Satellite       |18.0|8        |318.0       |150.0     |3436.0|11.0        |70   |US    |1           |\n",
            "|AMC Rebel SST            |16.0|8        |304.0       |150.0     |3433.0|12.0        |70   |US    |1           |\n",
            "|Ford Torino              |17.0|8        |302.0       |140.0     |3449.0|10.5        |70   |US    |1           |\n",
            "+-------------------------+----+---------+------------+----------+------+------------+-----+------+------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U9772_mHwAqL",
        "outputId": "1c3e21ff-346f-4169-93df-465db537de53"
      },
      "source": [
        "# ПРИМЕР 2: Добавление нескольких столбцов\n",
        "# Мы добавим два новых столбца под названием 'second_column' и 'third_column' в конце\n",
        "df = df.withColumn('second_column', lit(2)) \\\n",
        "       .withColumn('third_column', lit('Third Column'))\n",
        "# lit означает литерал. Он заполняет строку заданным буквальным значением.\n",
        "# При добавлении статических данных / постоянных значений, это хорошая практика, чтобы использовать его.\n",
        "df.show(5,truncate=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+---+------+-------------+------------+\n",
            "|name |age|height|second_column|third_column|\n",
            "+-----+---+------+-------------+------------+\n",
            "|Alice|5  |80    |2            |Third Column|\n",
            "|Jacob|24 |80    |2            |Third Column|\n",
            "|Alice|5  |80    |2            |Third Column|\n",
            "+-----+---+------+-------------+------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dGaQS_pOwx_b",
        "outputId": "46ebd54a-bec7-4a2e-b23e-2db7aa4e122c"
      },
      "source": [
        "# ПРИМЕР 3: Получение нового столбца из существующего\n",
        "# Мы добавим новый столбец под названием 'car_model', в котором значения car и model будут добавлены вместе с пробелом между ними.\n",
        "from pyspark.sql.functions import concat\n",
        "df = df.withColumn('car_model', concat(col(\"Car\"), lit(\" \"), col(\"model\")))\n",
        "# lit означает литерал. Он заполняет строку заданным буквальным значением.\n",
        "# При добавлении статических данных / постоянных значений, это хорошая практика, чтобы использовать его.\n",
        "df.show(5,truncate=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------------------+----+---------+------------+----------+------+------------+-----+------+------------+-------------+------------+----------------------------+\n",
            "|Car                      |MPG |Cylinders|Displacement|Horsepower|Weight|Acceleration|Model|Origin|first_column|second_column|third_column|car_model                   |\n",
            "+-------------------------+----+---------+------------+----------+------+------------+-----+------+------------+-------------+------------+----------------------------+\n",
            "|Chevrolet Chevelle Malibu|18.0|8        |307.0       |130.0     |3504.0|12.0        |70   |US    |1           |2            |Third Column|Chevrolet Chevelle Malibu 70|\n",
            "|Buick Skylark 320        |15.0|8        |350.0       |165.0     |3693.0|11.5        |70   |US    |1           |2            |Third Column|Buick Skylark 320 70        |\n",
            "|Plymouth Satellite       |18.0|8        |318.0       |150.0     |3436.0|11.0        |70   |US    |1           |2            |Third Column|Plymouth Satellite 70       |\n",
            "|AMC Rebel SST            |16.0|8        |304.0       |150.0     |3433.0|12.0        |70   |US    |1           |2            |Third Column|AMC Rebel SST 70            |\n",
            "|Ford Torino              |17.0|8        |302.0       |140.0     |3449.0|10.5        |70   |US    |1           |2            |Third Column|Ford Torino 70              |\n",
            "+-------------------------+----+---------+------------+----------+------+------------+-----+------+------------+-------------+------------+----------------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xeHExg-zxf5r"
      },
      "source": [
        "Как мы видим, новый столбец car model был создан из существующих столбцов. Поскольку нашей целью было создать столбец, в котором значения car и model были бы сложены вместе с пробелом между ними, мы использовали оператор `concat`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QlMf04i2CjDC"
      },
      "source": [
        "<a id='renaming-columns'></a>\n",
        "### Переименование столбцов"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CwGKbSHvxxxG"
      },
      "source": [
        "Для переименования столбца в PySpark мы используем функцию `withColumnRenamed`. Ниже мы увидим ее в действии:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QJqgy6lKfk2o",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "db6f187c-cb54-4a55-e4b3-98cbf589f447"
      },
      "source": [
        "#Переименование столбца в PySpark\n",
        "df = df.withColumnRenamed('first_column', 'new_column_one') \\\n",
        "       .withColumnRenamed('second_column', 'new_column_two') \\\n",
        "       .withColumnRenamed('third_column', 'new_column_three')\n",
        "df.show(truncate=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------------------+----+---------+------------+----------+------+------------+-----+------+--------------+--------------+----------------+-----------------------------------+\n",
            "|Car                             |MPG |Cylinders|Displacement|Horsepower|Weight|Acceleration|Model|Origin|new_column_one|new_column_two|new_column_three|car_model                          |\n",
            "+--------------------------------+----+---------+------------+----------+------+------------+-----+------+--------------+--------------+----------------+-----------------------------------+\n",
            "|Chevrolet Chevelle Malibu       |18.0|8        |307.0       |130.0     |3504.0|12.0        |70   |US    |1             |2             |Third Column    |Chevrolet Chevelle Malibu 70       |\n",
            "|Buick Skylark 320               |15.0|8        |350.0       |165.0     |3693.0|11.5        |70   |US    |1             |2             |Third Column    |Buick Skylark 320 70               |\n",
            "|Plymouth Satellite              |18.0|8        |318.0       |150.0     |3436.0|11.0        |70   |US    |1             |2             |Third Column    |Plymouth Satellite 70              |\n",
            "|AMC Rebel SST                   |16.0|8        |304.0       |150.0     |3433.0|12.0        |70   |US    |1             |2             |Third Column    |AMC Rebel SST 70                   |\n",
            "|Ford Torino                     |17.0|8        |302.0       |140.0     |3449.0|10.5        |70   |US    |1             |2             |Third Column    |Ford Torino 70                     |\n",
            "|Ford Galaxie 500                |15.0|8        |429.0       |198.0     |4341.0|10.0        |70   |US    |1             |2             |Third Column    |Ford Galaxie 500 70                |\n",
            "|Chevrolet Impala                |14.0|8        |454.0       |220.0     |4354.0|9.0         |70   |US    |1             |2             |Third Column    |Chevrolet Impala 70                |\n",
            "|Plymouth Fury iii               |14.0|8        |440.0       |215.0     |4312.0|8.5         |70   |US    |1             |2             |Third Column    |Plymouth Fury iii 70               |\n",
            "|Pontiac Catalina                |14.0|8        |455.0       |225.0     |4425.0|10.0        |70   |US    |1             |2             |Third Column    |Pontiac Catalina 70                |\n",
            "|AMC Ambassador DPL              |15.0|8        |390.0       |190.0     |3850.0|8.5         |70   |US    |1             |2             |Third Column    |AMC Ambassador DPL 70              |\n",
            "|Citroen DS-21 Pallas            |0.0 |4        |133.0       |115.0     |3090.0|17.5        |70   |Europe|1             |2             |Third Column    |Citroen DS-21 Pallas 70            |\n",
            "|Chevrolet Chevelle Concours (sw)|0.0 |8        |350.0       |165.0     |4142.0|11.5        |70   |US    |1             |2             |Third Column    |Chevrolet Chevelle Concours (sw) 70|\n",
            "|Ford Torino (sw)                |0.0 |8        |351.0       |153.0     |4034.0|11.0        |70   |US    |1             |2             |Third Column    |Ford Torino (sw) 70                |\n",
            "|Plymouth Satellite (sw)         |0.0 |8        |383.0       |175.0     |4166.0|10.5        |70   |US    |1             |2             |Third Column    |Plymouth Satellite (sw) 70         |\n",
            "|AMC Rebel SST (sw)              |0.0 |8        |360.0       |175.0     |3850.0|11.0        |70   |US    |1             |2             |Third Column    |AMC Rebel SST (sw) 70              |\n",
            "|Dodge Challenger SE             |15.0|8        |383.0       |170.0     |3563.0|10.0        |70   |US    |1             |2             |Third Column    |Dodge Challenger SE 70             |\n",
            "|Plymouth 'Cuda 340              |14.0|8        |340.0       |160.0     |3609.0|8.0         |70   |US    |1             |2             |Third Column    |Plymouth 'Cuda 340 70              |\n",
            "|Ford Mustang Boss 302           |0.0 |8        |302.0       |140.0     |3353.0|8.0         |70   |US    |1             |2             |Third Column    |Ford Mustang Boss 302 70           |\n",
            "|Chevrolet Monte Carlo           |15.0|8        |400.0       |150.0     |3761.0|9.5         |70   |US    |1             |2             |Third Column    |Chevrolet Monte Carlo 70           |\n",
            "|Buick Estate Wagon (sw)         |14.0|8        |455.0       |225.0     |3086.0|10.0        |70   |US    |1             |2             |Third Column    |Buick Estate Wagon (sw) 70         |\n",
            "+--------------------------------+----+---------+------------+----------+------+------------+-----+------+--------------+--------------+----------------+-----------------------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4CDifVC2Cnml"
      },
      "source": [
        "<a id='grouping-by-columns'></a>\n",
        "### Группировка по столбцам"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9wlB76FdyS0W"
      },
      "source": [
        "Здесь мы видим способ группировки значений с помощью Dataframe API. Мы обсудим, как:\n",
        "\n",
        "1.   Группировать по одному столбцу\n",
        "2.   Группировать по нескольким столбцам"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M1ek2opVfqea",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ca9212c1-d6e7-478e-dfd7-1262ee8482c3"
      },
      "source": [
        "# Группировка по столбцу в PySpark\n",
        "df.groupBy('Origin').count().show(5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+-----+\n",
            "|Origin|count|\n",
            "+------+-----+\n",
            "|Europe|   73|\n",
            "|    US|  254|\n",
            "| Japan|   79|\n",
            "+------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hUh_TWcOysoL",
        "outputId": "a7ece690-642b-40cd-b9b4-7ea1c8c3d828"
      },
      "source": [
        "# Группировка по нескольким столбцам в PySpark\n",
        "df.groupBy('Origin', 'Model').count().show(5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+-----+-----+\n",
            "|Origin|Model|count|\n",
            "+------+-----+-----+\n",
            "|Europe|   71|    5|\n",
            "|Europe|   80|    9|\n",
            "|Europe|   79|    4|\n",
            "| Japan|   75|    4|\n",
            "|    US|   72|   18|\n",
            "+------+-----+-----+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CbpEj9fECrW3"
      },
      "source": [
        "<a id='removing-columns'></a>\n",
        "### Удаление столбцов"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xsb9PXxpfnmh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "07d4e800-4386-4fe7-e372-886e46056a2f"
      },
      "source": [
        "#Удаление столбцов в PySpark\n",
        "df = df.drop('new_column_one')\n",
        "df.show(5,truncate=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------------------+----+---------+------------+----------+------+------------+-----+------+--------------+----------------+----------------------------+\n",
            "|Car                      |MPG |Cylinders|Displacement|Horsepower|Weight|Acceleration|Model|Origin|new_column_two|new_column_three|car_model                   |\n",
            "+-------------------------+----+---------+------------+----------+------+------------+-----+------+--------------+----------------+----------------------------+\n",
            "|Chevrolet Chevelle Malibu|18.0|8        |307.0       |130.0     |3504.0|12.0        |70   |US    |2             |Third Column    |Chevrolet Chevelle Malibu 70|\n",
            "|Buick Skylark 320        |15.0|8        |350.0       |165.0     |3693.0|11.5        |70   |US    |2             |Third Column    |Buick Skylark 320 70        |\n",
            "|Plymouth Satellite       |18.0|8        |318.0       |150.0     |3436.0|11.0        |70   |US    |2             |Third Column    |Plymouth Satellite 70       |\n",
            "|AMC Rebel SST            |16.0|8        |304.0       |150.0     |3433.0|12.0        |70   |US    |2             |Third Column    |AMC Rebel SST 70            |\n",
            "|Ford Torino              |17.0|8        |302.0       |140.0     |3449.0|10.5        |70   |US    |2             |Third Column    |Ford Torino 70              |\n",
            "+-------------------------+----+---------+------------+----------+------+------------+-----+------+--------------+----------------+----------------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EKOXrXtvzK_0",
        "outputId": "91e85922-f406-44d1-867c-12e95b947701"
      },
      "source": [
        "#Удалите несколько колонок за один раз\n",
        "df = df.drop('new_column_two') \\\n",
        "       .drop('new_column_three')\n",
        "df.show(5,truncate=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------------------+----+---------+------------+----------+------+------------+-----+------+----------------------------+\n",
            "|Car                      |MPG |Cylinders|Displacement|Horsepower|Weight|Acceleration|Model|Origin|car_model                   |\n",
            "+-------------------------+----+---------+------------+----------+------+------------+-----+------+----------------------------+\n",
            "|Chevrolet Chevelle Malibu|18.0|8        |307.0       |130.0     |3504.0|12.0        |70   |US    |Chevrolet Chevelle Malibu 70|\n",
            "|Buick Skylark 320        |15.0|8        |350.0       |165.0     |3693.0|11.5        |70   |US    |Buick Skylark 320 70        |\n",
            "|Plymouth Satellite       |18.0|8        |318.0       |150.0     |3436.0|11.0        |70   |US    |Plymouth Satellite 70       |\n",
            "|AMC Rebel SST            |16.0|8        |304.0       |150.0     |3433.0|12.0        |70   |US    |AMC Rebel SST 70            |\n",
            "|Ford Torino              |17.0|8        |302.0       |140.0     |3449.0|10.5        |70   |US    |Ford Torino 70              |\n",
            "+-------------------------+----+---------+------------+----------+------+------------+-----+------+----------------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WbKK5iHwmIoV"
      },
      "source": [
        "<a id='dataframe-operations-on-rows'></a>\n",
        "## Операции над строками фрейма данных"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Quwx3KlLzeq9"
      },
      "source": [
        "В этом разделе мы рассмотрим следующие вопросы:\n",
        "\n",
        "1.   Фильтрация строк\n",
        "2. \t Получение отличительных строк\n",
        "3.   Сортировка строк\n",
        "4.   Объединение фреймов данных\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9bKlvX-SH-Wy"
      },
      "source": [
        "<a id='filtering-rows'></a>\n",
        "### Фильтрация строк"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YNfcjOIknA3n",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "18b4a083-cae6-4f9d-80fd-d734e498ffa9"
      },
      "source": [
        "# Filtering rows in PySpark\n",
        "total_count = df.count()\n",
        "print(\"TOTAL RECORD COUNT: \" + str(total_count))\n",
        "europe_filtered_count = df.filter(col('Origin')=='Europe').count()\n",
        "print(\"EUROPE FILTERED RECORD COUNT: \" + str(europe_filtered_count))\n",
        "df.filter(col('Origin')=='Europe').show(truncate=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TOTAL RECORD COUNT: 406\n",
            "EUROPE FILTERED RECORD COUNT: 73\n",
            "+----------------------------+----+---------+------------+----------+------+------------+-----+------+-------------------------------+\n",
            "|Car                         |MPG |Cylinders|Displacement|Horsepower|Weight|Acceleration|Model|Origin|car_model                      |\n",
            "+----------------------------+----+---------+------------+----------+------+------------+-----+------+-------------------------------+\n",
            "|Citroen DS-21 Pallas        |0.0 |4        |133.0       |115.0     |3090.0|17.5        |70   |Europe|Citroen DS-21 Pallas 70        |\n",
            "|Volkswagen 1131 Deluxe Sedan|26.0|4        |97.0        |46.0      |1835.0|20.5        |70   |Europe|Volkswagen 1131 Deluxe Sedan 70|\n",
            "|Peugeot 504                 |25.0|4        |110.0       |87.0      |2672.0|17.5        |70   |Europe|Peugeot 504 70                 |\n",
            "|Audi 100 LS                 |24.0|4        |107.0       |90.0      |2430.0|14.5        |70   |Europe|Audi 100 LS 70                 |\n",
            "|Saab 99e                    |25.0|4        |104.0       |95.0      |2375.0|17.5        |70   |Europe|Saab 99e 70                    |\n",
            "|BMW 2002                    |26.0|4        |121.0       |113.0     |2234.0|12.5        |70   |Europe|BMW 2002 70                    |\n",
            "|Volkswagen Super Beetle 117 |0.0 |4        |97.0        |48.0      |1978.0|20.0        |71   |Europe|Volkswagen Super Beetle 117 71 |\n",
            "|Opel 1900                   |28.0|4        |116.0       |90.0      |2123.0|14.0        |71   |Europe|Opel 1900 71                   |\n",
            "|Peugeot 304                 |30.0|4        |79.0        |70.0      |2074.0|19.5        |71   |Europe|Peugeot 304 71                 |\n",
            "|Fiat 124B                   |30.0|4        |88.0        |76.0      |2065.0|14.5        |71   |Europe|Fiat 124B 71                   |\n",
            "|Volkswagen Model 111        |27.0|4        |97.0        |60.0      |1834.0|19.0        |71   |Europe|Volkswagen Model 111 71        |\n",
            "|Volkswagen Type 3           |23.0|4        |97.0        |54.0      |2254.0|23.5        |72   |Europe|Volkswagen Type 3 72           |\n",
            "|Volvo 145e (sw)             |18.0|4        |121.0       |112.0     |2933.0|14.5        |72   |Europe|Volvo 145e (sw) 72             |\n",
            "|Volkswagen 411 (sw)         |22.0|4        |121.0       |76.0      |2511.0|18.0        |72   |Europe|Volkswagen 411 (sw) 72         |\n",
            "|Peugeot 504 (sw)            |21.0|4        |120.0       |87.0      |2979.0|19.5        |72   |Europe|Peugeot 504 (sw) 72            |\n",
            "|Renault 12 (sw)             |26.0|4        |96.0        |69.0      |2189.0|18.0        |72   |Europe|Renault 12 (sw) 72             |\n",
            "|Volkswagen Super Beetle     |26.0|4        |97.0        |46.0      |1950.0|21.0        |73   |Europe|Volkswagen Super Beetle 73     |\n",
            "|Fiat 124 Sport Coupe        |26.0|4        |98.0        |90.0      |2265.0|15.5        |73   |Europe|Fiat 124 Sport Coupe 73        |\n",
            "|Fiat 128                    |29.0|4        |68.0        |49.0      |1867.0|19.5        |73   |Europe|Fiat 128 73                    |\n",
            "|Opel Manta                  |24.0|4        |116.0       |75.0      |2158.0|15.5        |73   |Europe|Opel Manta 73                  |\n",
            "+----------------------------+----+---------+------------+----------+------+------------+-----+------+-------------------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MXJxRwBQ1lyd",
        "outputId": "e3fee4b0-965e-4669-e76d-b9dd649a508d"
      },
      "source": [
        "# Filtering rows in PySpark based on Multiple conditions\n",
        "total_count = df.count()\n",
        "print(\"TOTAL RECORD COUNT: \" + str(total_count))\n",
        "europe_filtered_count = df.filter((col('Origin')=='Europe') &\n",
        "                                  (col('Cylinders')==4)).count() # Two conditions added here\n",
        "print(\"EUROPE FILTERED RECORD COUNT: \" + str(europe_filtered_count))\n",
        "df.filter(col('Origin')=='Europe').show(truncate=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TOTAL RECORD COUNT: 406\n",
            "EUROPE FILTERED RECORD COUNT: 66\n",
            "+----------------------------+----+---------+------------+----------+------+------------+-----+------+-------------------------------+\n",
            "|Car                         |MPG |Cylinders|Displacement|Horsepower|Weight|Acceleration|Model|Origin|car_model                      |\n",
            "+----------------------------+----+---------+------------+----------+------+------------+-----+------+-------------------------------+\n",
            "|Citroen DS-21 Pallas        |0.0 |4        |133.0       |115.0     |3090.0|17.5        |70   |Europe|Citroen DS-21 Pallas 70        |\n",
            "|Volkswagen 1131 Deluxe Sedan|26.0|4        |97.0        |46.0      |1835.0|20.5        |70   |Europe|Volkswagen 1131 Deluxe Sedan 70|\n",
            "|Peugeot 504                 |25.0|4        |110.0       |87.0      |2672.0|17.5        |70   |Europe|Peugeot 504 70                 |\n",
            "|Audi 100 LS                 |24.0|4        |107.0       |90.0      |2430.0|14.5        |70   |Europe|Audi 100 LS 70                 |\n",
            "|Saab 99e                    |25.0|4        |104.0       |95.0      |2375.0|17.5        |70   |Europe|Saab 99e 70                    |\n",
            "|BMW 2002                    |26.0|4        |121.0       |113.0     |2234.0|12.5        |70   |Europe|BMW 2002 70                    |\n",
            "|Volkswagen Super Beetle 117 |0.0 |4        |97.0        |48.0      |1978.0|20.0        |71   |Europe|Volkswagen Super Beetle 117 71 |\n",
            "|Opel 1900                   |28.0|4        |116.0       |90.0      |2123.0|14.0        |71   |Europe|Opel 1900 71                   |\n",
            "|Peugeot 304                 |30.0|4        |79.0        |70.0      |2074.0|19.5        |71   |Europe|Peugeot 304 71                 |\n",
            "|Fiat 124B                   |30.0|4        |88.0        |76.0      |2065.0|14.5        |71   |Europe|Fiat 124B 71                   |\n",
            "|Volkswagen Model 111        |27.0|4        |97.0        |60.0      |1834.0|19.0        |71   |Europe|Volkswagen Model 111 71        |\n",
            "|Volkswagen Type 3           |23.0|4        |97.0        |54.0      |2254.0|23.5        |72   |Europe|Volkswagen Type 3 72           |\n",
            "|Volvo 145e (sw)             |18.0|4        |121.0       |112.0     |2933.0|14.5        |72   |Europe|Volvo 145e (sw) 72             |\n",
            "|Volkswagen 411 (sw)         |22.0|4        |121.0       |76.0      |2511.0|18.0        |72   |Europe|Volkswagen 411 (sw) 72         |\n",
            "|Peugeot 504 (sw)            |21.0|4        |120.0       |87.0      |2979.0|19.5        |72   |Europe|Peugeot 504 (sw) 72            |\n",
            "|Renault 12 (sw)             |26.0|4        |96.0        |69.0      |2189.0|18.0        |72   |Europe|Renault 12 (sw) 72             |\n",
            "|Volkswagen Super Beetle     |26.0|4        |97.0        |46.0      |1950.0|21.0        |73   |Europe|Volkswagen Super Beetle 73     |\n",
            "|Fiat 124 Sport Coupe        |26.0|4        |98.0        |90.0      |2265.0|15.5        |73   |Europe|Fiat 124 Sport Coupe 73        |\n",
            "|Fiat 128                    |29.0|4        |68.0        |49.0      |1867.0|19.5        |73   |Europe|Fiat 128 73                    |\n",
            "|Opel Manta                  |24.0|4        |116.0       |75.0      |2158.0|15.5        |73   |Europe|Opel Manta 73                  |\n",
            "+----------------------------+----+---------+------------+----------+------+------------+-----+------+-------------------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zLU-a4auIEvh"
      },
      "source": [
        "<a id='get-distinct-rows'></a>\n",
        "### Получить отличительные строки"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B1RKg1UrmBQz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e97af188-f964-4384-9360-a48a296979de"
      },
      "source": [
        "#Get Unique Rows in PySpark\n",
        "df.select('Origin').distinct().show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+\n",
            "|Origin|\n",
            "+------+\n",
            "|Europe|\n",
            "|    US|\n",
            "| Japan|\n",
            "+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_LQWXPXt0g0N",
        "outputId": "0efb21f1-bc7c-4c09-a51d-e2493373005c"
      },
      "source": [
        "#Get Unique Rows in PySpark based on mutliple columns\n",
        "df.select('Origin','model').distinct().show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+-----+\n",
            "|Origin|model|\n",
            "+------+-----+\n",
            "|Europe|   71|\n",
            "|Europe|   80|\n",
            "|Europe|   79|\n",
            "| Japan|   75|\n",
            "|    US|   72|\n",
            "|    US|   80|\n",
            "|Europe|   74|\n",
            "| Japan|   79|\n",
            "|Europe|   76|\n",
            "|    US|   75|\n",
            "| Japan|   77|\n",
            "|    US|   82|\n",
            "| Japan|   80|\n",
            "| Japan|   78|\n",
            "|    US|   78|\n",
            "|Europe|   75|\n",
            "|    US|   71|\n",
            "|    US|   77|\n",
            "| Japan|   70|\n",
            "| Japan|   71|\n",
            "+------+-----+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-069UYUwIIYI"
      },
      "source": [
        "<a id='sorting-rows'></a>\n",
        "### Сортировка строк"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ZpeJvz0nkBI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "75060c3e-3701-47a9-c559-a78c98bc4738"
      },
      "source": [
        "# Sort Rows in PySpark\n",
        "# By default the data will be sorted in ascending order\n",
        "df.orderBy('Cylinders').show(truncate=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------------------------+----+---------+------------+----------+------+------------+-----+------+-------------------------------+\n",
            "|Car                         |MPG |Cylinders|Displacement|Horsepower|Weight|Acceleration|Model|Origin|car_model                      |\n",
            "+----------------------------+----+---------+------------+----------+------+------------+-----+------+-------------------------------+\n",
            "|Mazda RX-4                  |21.5|3        |80.0        |110.0     |2720.0|13.5        |77   |Japan |Mazda RX-4 77                  |\n",
            "|Mazda RX-7 GS               |23.7|3        |70.0        |100.0     |2420.0|12.5        |80   |Japan |Mazda RX-7 GS 80               |\n",
            "|Mazda RX2 Coupe             |19.0|3        |70.0        |97.0      |2330.0|13.5        |72   |Japan |Mazda RX2 Coupe 72             |\n",
            "|Mazda RX3                   |18.0|3        |70.0        |90.0      |2124.0|13.5        |73   |Japan |Mazda RX3 73                   |\n",
            "|Datsun 510 (sw)             |28.0|4        |97.0        |92.0      |2288.0|17.0        |72   |Japan |Datsun 510 (sw) 72             |\n",
            "|Opel 1900                   |28.0|4        |116.0       |90.0      |2123.0|14.0        |71   |Europe|Opel 1900 71                   |\n",
            "|Mercury Capri 2000          |23.0|4        |122.0       |86.0      |2220.0|14.0        |71   |US    |Mercury Capri 2000 71          |\n",
            "|Volkswagen 1131 Deluxe Sedan|26.0|4        |97.0        |46.0      |1835.0|20.5        |70   |Europe|Volkswagen 1131 Deluxe Sedan 70|\n",
            "|Peugeot 304                 |30.0|4        |79.0        |70.0      |2074.0|19.5        |71   |Europe|Peugeot 304 71                 |\n",
            "|Fiat 124B                   |30.0|4        |88.0        |76.0      |2065.0|14.5        |71   |Europe|Fiat 124B 71                   |\n",
            "|Chevrolet Vega (sw)         |22.0|4        |140.0       |72.0      |2408.0|19.0        |71   |US    |Chevrolet Vega (sw) 71         |\n",
            "|Datsun 1200                 |35.0|4        |72.0        |69.0      |1613.0|18.0        |71   |Japan |Datsun 1200 71                 |\n",
            "|Volkswagen Model 111        |27.0|4        |97.0        |60.0      |1834.0|19.0        |71   |Europe|Volkswagen Model 111 71        |\n",
            "|Volkswagen Type 3           |23.0|4        |97.0        |54.0      |2254.0|23.5        |72   |Europe|Volkswagen Type 3 72           |\n",
            "|Audi 100 LS                 |24.0|4        |107.0       |90.0      |2430.0|14.5        |70   |Europe|Audi 100 LS 70                 |\n",
            "|BMW 2002                    |26.0|4        |121.0       |113.0     |2234.0|12.5        |70   |Europe|BMW 2002 70                    |\n",
            "|Toyota Corolla 1200         |31.0|4        |71.0        |65.0      |1773.0|19.0        |71   |Japan |Toyota Corolla 1200 71         |\n",
            "|Chevrolet Vega 2300         |28.0|4        |140.0       |90.0      |2264.0|15.5        |71   |US    |Chevrolet Vega 2300 71         |\n",
            "|Ford Pinto                  |25.0|4        |98.0        |0.0       |2046.0|19.0        |71   |US    |Ford Pinto 71                  |\n",
            "|Dodge Colt Hardtop          |25.0|4        |97.5        |80.0      |2126.0|17.0        |72   |US    |Dodge Colt Hardtop 72          |\n",
            "+----------------------------+----+---------+------------+----------+------+------------+-----+------+-------------------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v1CEwofMJV-D",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "987f7ab6-501a-43a4-a145-54516587055e"
      },
      "source": [
        "# To change the sorting order, you can use the ascending parameter\n",
        "df.orderBy('Cylinders', ascending=False).show(truncate=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------------------+----+---------+------------+----------+------+------------+-----+------+----------------------------+\n",
            "|Car                      |MPG |Cylinders|Displacement|Horsepower|Weight|Acceleration|Model|Origin|car_model                   |\n",
            "+-------------------------+----+---------+------------+----------+------+------------+-----+------+----------------------------+\n",
            "|Plymouth 'Cuda 340       |14.0|8        |340.0       |160.0     |3609.0|8.0         |70   |US    |Plymouth 'Cuda 340 70       |\n",
            "|Pontiac Safari (sw)      |13.0|8        |400.0       |175.0     |5140.0|12.0        |71   |US    |Pontiac Safari (sw) 71      |\n",
            "|Ford Mustang Boss 302    |0.0 |8        |302.0       |140.0     |3353.0|8.0         |70   |US    |Ford Mustang Boss 302 70    |\n",
            "|Buick Skylark 320        |15.0|8        |350.0       |165.0     |3693.0|11.5        |70   |US    |Buick Skylark 320 70        |\n",
            "|Chevrolet Monte Carlo    |15.0|8        |400.0       |150.0     |3761.0|9.5         |70   |US    |Chevrolet Monte Carlo 70    |\n",
            "|AMC Rebel SST            |16.0|8        |304.0       |150.0     |3433.0|12.0        |70   |US    |AMC Rebel SST 70            |\n",
            "|Buick Estate Wagon (sw)  |14.0|8        |455.0       |225.0     |3086.0|10.0        |70   |US    |Buick Estate Wagon (sw) 70  |\n",
            "|Ford Galaxie 500         |15.0|8        |429.0       |198.0     |4341.0|10.0        |70   |US    |Ford Galaxie 500 70         |\n",
            "|Ford F250                |10.0|8        |360.0       |215.0     |4615.0|14.0        |70   |US    |Ford F250 70                |\n",
            "|Plymouth Fury iii        |14.0|8        |440.0       |215.0     |4312.0|8.5         |70   |US    |Plymouth Fury iii 70        |\n",
            "|Chevy C20                |10.0|8        |307.0       |200.0     |4376.0|15.0        |70   |US    |Chevy C20 70                |\n",
            "|AMC Ambassador DPL       |15.0|8        |390.0       |190.0     |3850.0|8.5         |70   |US    |AMC Ambassador DPL 70       |\n",
            "|Dodge D200               |11.0|8        |318.0       |210.0     |4382.0|13.5        |70   |US    |Dodge D200 70               |\n",
            "|Ford Torino (sw)         |0.0 |8        |351.0       |153.0     |4034.0|11.0        |70   |US    |Ford Torino (sw) 70         |\n",
            "|Hi 1200D                 |9.0 |8        |304.0       |193.0     |4732.0|18.5        |70   |US    |Hi 1200D 70                 |\n",
            "|AMC Rebel SST (sw)       |0.0 |8        |360.0       |175.0     |3850.0|11.0        |70   |US    |AMC Rebel SST (sw) 70       |\n",
            "|Chevrolet Impala         |14.0|8        |350.0       |165.0     |4209.0|12.0        |71   |US    |Chevrolet Impala 71         |\n",
            "|Chevrolet Chevelle Malibu|18.0|8        |307.0       |130.0     |3504.0|12.0        |70   |US    |Chevrolet Chevelle Malibu 70|\n",
            "|Pontiac Catalina Brougham|14.0|8        |400.0       |175.0     |4464.0|11.5        |71   |US    |Pontiac Catalina Brougham 71|\n",
            "|Ford Torino              |17.0|8        |302.0       |140.0     |3449.0|10.5        |70   |US    |Ford Torino 70              |\n",
            "+-------------------------+----+---------+------------+----------+------+------------+-----+------+----------------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zx3W4aeL5A4O",
        "outputId": "7b8bd9a4-2240-4637-eb59-18693c2ac028"
      },
      "source": [
        "# Using groupBy aand orderBy together\n",
        "df.groupBy(\"Origin\").count().orderBy('count', ascending=False).show(10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+-----+\n",
            "|Origin|count|\n",
            "+------+-----+\n",
            "|    US|  254|\n",
            "| Japan|   79|\n",
            "|Europe|   73|\n",
            "+------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aN0-A_JsIX-X"
      },
      "source": [
        "<a id='union-dataframes'></a>\n",
        "### Объединение кадров данных"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VH0KOaBrJt6v"
      },
      "source": [
        "Вы увидите три основных метода для выполнения объединения фреймов данных. Важно знать, в чем разница между ними и какой из них предпочтительнее:\n",
        "\n",
        "* `union()` - Используется для объединения двух DataFrames с одинаковой структурой/схемой. Если схемы не совпадают, возвращается ошибка\n",
        "* `unionAll()` - Эта функция устарела с версии Spark 2.0.0 и заменена на union()\n",
        "* `unionByName()` - Эта функция используется для объединения двух датафреймов на основе имени столбца.\n",
        "\n",
        "> Поскольку функция `unionAll()` устарела, **`union()` является предпочтительным методом объединения фреймов данных.** <br> > Разница между `unionByName()` и `union()` заключается в том, что `unionByName()` разрешает столбцы по имени, а не по позиции.\n",
        "\n",
        "В других SQL, Union удаляет дубликаты, а UnionAll объединяет два набора данных, тем самым включая дубликаты записей. Но в PySpark оба варианта ведут себя одинаково и включают дубликаты записей. Рекомендуется использовать `distinct()` или `dropDuplicates()` для удаления дубликатов."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bCZIzfYmnx--",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a4a0e30a-be9a-4fb6-bb6d-ee785463d56d"
      },
      "source": [
        "# CASE 1: Union When columns are in order\n",
        "df = spark.read.csv('cars.csv', header=True, sep=\";\", inferSchema=True)\n",
        "europe_cars = df.filter((col('Origin')=='Europe') & (col('Cylinders')==5))\n",
        "japan_cars = df.filter((col('Origin')=='Japan') & (col('Cylinders')==3))\n",
        "print(\"EUROPE CARS: \"+str(europe_cars.count()))\n",
        "print(\"JAPAN CARS: \"+str(japan_cars.count()))\n",
        "print(\"AFTER UNION: \"+str(europe_cars.union(japan_cars).count()))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EUROPE CARS: 3\n",
            "JAPAN CARS: 4\n",
            "AFTER UNION: 7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1pfPzVOFqC_8"
      },
      "source": [
        "**Результат:**\n",
        "\n",
        "> Как вы можете видеть здесь, было 3 автомобиля из Европы с 5 цилиндрами, и 4 автомобиля из Японии с 3 цилиндрами. После объединения осталось 7 автомобилей."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CjWjzWBoMxx0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9d41f725-cbf8-49d0-c3b0-9f92ed04032d"
      },
      "source": [
        "# CASE 1: Union When columns are not in order\n",
        "# Creating two dataframes with jumbled columns\n",
        "df1 = spark.createDataFrame([[1, 2, 3]], [\"col0\", \"col1\", \"col2\"])\n",
        "df2 = spark.createDataFrame([[4, 5, 6]], [\"col1\", \"col2\", \"col0\"])\n",
        "df1.unionByName(df2).show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+----+----+\n",
            "|col0|col1|col2|\n",
            "+----+----+----+\n",
            "|   1|   2|   3|\n",
            "|   6|   4|   5|\n",
            "+----+----+----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EX33t8e3PyGy"
      },
      "source": [
        "**Результат:**\n",
        "\n",
        "> Как вы можете видеть здесь, два кадра данных были успешно объединены на основе имен их столбцов."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aHjILb1DriuX"
      },
      "source": [
        "<a id='common-data-manipulation-functions'></a>\n",
        "## Общие функции манипулирования данными"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x3vlC7ZerlKb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b65f1d39-f5d2-4eb8-fb36-613af74eb39f"
      },
      "source": [
        "# Functions available in PySpark\n",
        "from pyspark.sql import functions\n",
        "# Similar to python, we can use the dir function to view the avaiable functions\n",
        "print(dir(functions))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Column', 'DataFrame', 'DataType', 'PandasUDFType', 'PythonEvalType', 'SparkContext', 'StringType', 'UserDefinedFunction', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_create_column_from_literal', '_create_lambda', '_create_udf', '_get_get_jvm_function', '_get_lambda_parameters', '_invoke_binary_math_function', '_invoke_function', '_invoke_function_over_column', '_invoke_higher_order_function', '_options_to_str', '_test', '_to_java_column', '_to_seq', '_unresolved_named_lambda_variable', 'abs', 'acos', 'acosh', 'add_months', 'aggregate', 'approxCountDistinct', 'approx_count_distinct', 'array', 'array_contains', 'array_distinct', 'array_except', 'array_intersect', 'array_join', 'array_max', 'array_min', 'array_position', 'array_remove', 'array_repeat', 'array_sort', 'array_union', 'arrays_overlap', 'arrays_zip', 'asc', 'asc_nulls_first', 'asc_nulls_last', 'ascii', 'asin', 'asinh', 'assert_true', 'atan', 'atan2', 'atanh', 'avg', 'base64', 'bin', 'bitwiseNOT', 'broadcast', 'bround', 'bucket', 'cbrt', 'ceil', 'coalesce', 'col', 'collect_list', 'collect_set', 'column', 'concat', 'concat_ws', 'conv', 'corr', 'cos', 'cosh', 'count', 'countDistinct', 'covar_pop', 'covar_samp', 'crc32', 'create_map', 'cume_dist', 'current_date', 'current_timestamp', 'date_add', 'date_format', 'date_sub', 'date_trunc', 'datediff', 'dayofmonth', 'dayofweek', 'dayofyear', 'days', 'decode', 'degrees', 'dense_rank', 'desc', 'desc_nulls_first', 'desc_nulls_last', 'element_at', 'encode', 'exists', 'exp', 'explode', 'explode_outer', 'expm1', 'expr', 'factorial', 'filter', 'first', 'flatten', 'floor', 'forall', 'format_number', 'format_string', 'from_csv', 'from_json', 'from_unixtime', 'from_utc_timestamp', 'functools', 'get_json_object', 'greatest', 'grouping', 'grouping_id', 'hash', 'hex', 'hour', 'hours', 'hypot', 'initcap', 'input_file_name', 'instr', 'isnan', 'isnull', 'json_tuple', 'kurtosis', 'lag', 'last', 'last_day', 'lead', 'least', 'length', 'levenshtein', 'lit', 'locate', 'log', 'log10', 'log1p', 'log2', 'lower', 'lpad', 'ltrim', 'map_concat', 'map_entries', 'map_filter', 'map_from_arrays', 'map_from_entries', 'map_keys', 'map_values', 'map_zip_with', 'max', 'md5', 'mean', 'min', 'minute', 'monotonically_increasing_id', 'month', 'months', 'months_between', 'nanvl', 'next_day', 'nth_value', 'ntile', 'overlay', 'pandas_udf', 'percent_rank', 'percentile_approx', 'posexplode', 'posexplode_outer', 'pow', 'quarter', 'radians', 'raise_error', 'rand', 'randn', 'rank', 'regexp_extract', 'regexp_replace', 'repeat', 'reverse', 'rint', 'round', 'row_number', 'rpad', 'rtrim', 'schema_of_csv', 'schema_of_json', 'second', 'sequence', 'sha1', 'sha2', 'shiftLeft', 'shiftRight', 'shiftRightUnsigned', 'shuffle', 'signum', 'sin', 'since', 'sinh', 'size', 'skewness', 'slice', 'sort_array', 'soundex', 'spark_partition_id', 'split', 'sqrt', 'stddev', 'stddev_pop', 'stddev_samp', 'struct', 'substring', 'substring_index', 'sum', 'sumDistinct', 'sys', 'tan', 'tanh', 'timestamp_seconds', 'toDegrees', 'toRadians', 'to_csv', 'to_date', 'to_json', 'to_str', 'to_timestamp', 'to_utc_timestamp', 'transform', 'transform_keys', 'transform_values', 'translate', 'trim', 'trunc', 'udf', 'unbase64', 'unhex', 'unix_timestamp', 'upper', 'var_pop', 'var_samp', 'variance', 'warnings', 'weekofyear', 'when', 'window', 'xxhash64', 'year', 'years', 'zip_with']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PIKigra7A34e"
      },
      "source": [
        "<a id='string-functions'></a>\n",
        "### Строковые функции"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "63QDccSjBqC4"
      },
      "source": [
        "# Loading the data\n",
        "from pyspark.sql.functions import col\n",
        "df = spark.read.csv('cars.csv', header=True, sep=\";\", inferSchema=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LiXWN8DUA9x6"
      },
      "source": [
        "**Отобразить колонку Car в виде существующих, нижних и верхних символов, а также первые 4 символа колонки**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "52Gh9c99BZFr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "02e07feb-8197-47e3-f95e-c7a4991e7e5a"
      },
      "source": [
        "from pyspark.sql.functions import col,lower, upper, substring\n",
        "# Prints out the details of a function\n",
        "help(substring)\n",
        "# alias is used to rename the column name in the output\n",
        "df.select(col('Car'),lower(col('Car')),upper(col('Car')),substring(col('Car'),1,4).alias(\"concatenated value\")).show(5, False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Help on function substring in module pyspark.sql.functions:\n",
            "\n",
            "substring(str, pos, len)\n",
            "    Substring starts at `pos` and is of length `len` when str is String type or\n",
            "    returns the slice of byte array that starts at `pos` in byte and is of length `len`\n",
            "    when str is Binary type.\n",
            "    \n",
            "    .. versionadded:: 1.5.0\n",
            "    \n",
            "    Notes\n",
            "    -----\n",
            "    The position is not zero based, but 1 based index.\n",
            "    \n",
            "    Examples\n",
            "    --------\n",
            "    >>> df = spark.createDataFrame([('abcd',)], ['s',])\n",
            "    >>> df.select(substring(df.s, 1, 2).alias('s')).collect()\n",
            "    [Row(s='ab')]\n",
            "\n",
            "+-------------------------+-------------------------+-------------------------+------------------+\n",
            "|Car                      |lower(Car)               |upper(Car)               |concatenated value|\n",
            "+-------------------------+-------------------------+-------------------------+------------------+\n",
            "|Chevrolet Chevelle Malibu|chevrolet chevelle malibu|CHEVROLET CHEVELLE MALIBU|Chev              |\n",
            "|Buick Skylark 320        |buick skylark 320        |BUICK SKYLARK 320        |Buic              |\n",
            "|Plymouth Satellite       |plymouth satellite       |PLYMOUTH SATELLITE       |Plym              |\n",
            "|AMC Rebel SST            |amc rebel sst            |AMC REBEL SST            |AMC               |\n",
            "|Ford Torino              |ford torino              |FORD TORINO              |Ford              |\n",
            "+-------------------------+-------------------------+-------------------------+------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GZJFdTbk6rBt"
      },
      "source": [
        "**Соедините столбец \"Автомобиль\" и столбец \"Модель\" и добавьте между ними пробел."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Lo951Cg6phi",
        "outputId": "672b8d88-246b-44e3-e4b8-46e6f3c5d8b5"
      },
      "source": [
        "from pyspark.sql.functions import concat\n",
        "df.select(col(\"Car\"),col(\"model\"),concat(col(\"Car\"), lit(\" \"), col(\"model\"))).show(5, False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------------------+-----+----------------------------+\n",
            "|Car                      |model|concat(Car,  , model)       |\n",
            "+-------------------------+-----+----------------------------+\n",
            "|Chevrolet Chevelle Malibu|70   |Chevrolet Chevelle Malibu 70|\n",
            "|Buick Skylark 320        |70   |Buick Skylark 320 70        |\n",
            "|Plymouth Satellite       |70   |Plymouth Satellite 70       |\n",
            "|AMC Rebel SST            |70   |AMC Rebel SST 70            |\n",
            "|Ford Torino              |70   |Ford Torino 70              |\n",
            "+-------------------------+-----+----------------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ldtA0wk9BMkT"
      },
      "source": [
        "<a id='numeric-functions'></a>\n",
        "### Числовые функции"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kmz4G5LVBOs6"
      },
      "source": [
        "**Показать самую старую и самую последнюю дату**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wBDDH-YpBbdk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3e480b1e-d05b-4962-c549-1f7f195e34b8"
      },
      "source": [
        "from pyspark.sql.functions import min, max\n",
        "df.select(min(col('Weight')), max(col('Weight'))).show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+-----------+\n",
            "|min(Weight)|max(Weight)|\n",
            "+-----------+-----------+\n",
            "|       1613|       5140|\n",
            "+-----------+-----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MTg-Royz7Nvi"
      },
      "source": [
        "**Добавьте 10 к минимальному и максимальному весу**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YeiemMsI7Vm2",
        "outputId": "cbf9a7aa-43ff-4760-f344-decf32667b78"
      },
      "source": [
        "from pyspark.sql.functions import min, max, lit\n",
        "df.select(min(col('Weight'))+lit(10), max(col('Weight')+lit(10))).show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------------+------------------+\n",
            "|(min(Weight) + 10)|max((Weight + 10))|\n",
            "+------------------+------------------+\n",
            "|              1623|              5150|\n",
            "+------------------+------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KQ6Ul9HGCwC3"
      },
      "source": [
        "<a id='operations-on-date'></a>\n",
        "### Operations on Date"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s1jmBN2qFHyk"
      },
      "source": [
        "> [PySpark follows SimpleDateFormat table of Java. Click here to view the docs.](https://docs.oracle.com/javase/7/docs/api/java/text/SimpleDateFormat.html)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sCTeI_JvDCsH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "61633ab8-2ea2-49b0-d513-6bad3d1fb11a"
      },
      "source": [
        "from pyspark.sql.functions import to_date, to_timestamp, lit\n",
        "df = spark.createDataFrame([('2019-12-25 13:30:00',)], ['DOB'])\n",
        "df.show()\n",
        "df.printSchema()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------------+\n",
            "|                DOB|\n",
            "+-------------------+\n",
            "|2019-12-25 13:30:00|\n",
            "+-------------------+\n",
            "\n",
            "root\n",
            " |-- DOB: string (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZH8ja1eHEW8x",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c4560dac-02e7-4665-c1e3-764a18df2434"
      },
      "source": [
        "df = spark.createDataFrame([('2019-12-25 13:30:00',)], ['DOB'])\n",
        "df = df.select(to_date(col('DOB'),'yyyy-MM-dd HH:mm:ss'), to_timestamp(col('DOB'),'yyyy-MM-dd HH:mm:ss'))\n",
        "df.show()\n",
        "df.printSchema()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------------------------------+--------------------------------------+\n",
            "|to_date(DOB, yyyy-MM-dd HH:mm:ss)|to_timestamp(DOB, yyyy-MM-dd HH:mm:ss)|\n",
            "+---------------------------------+--------------------------------------+\n",
            "|                       2019-12-25|                   2019-12-25 13:30:00|\n",
            "+---------------------------------+--------------------------------------+\n",
            "\n",
            "root\n",
            " |-- to_date(DOB, yyyy-MM-dd HH:mm:ss): date (nullable = true)\n",
            " |-- to_timestamp(DOB, yyyy-MM-dd HH:mm:ss): timestamp (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7g9m_8PPErI1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "62b5e49d-f688-4066-cdd2-ac1d796a98f5"
      },
      "source": [
        "df = spark.createDataFrame([('25/Dec/2019 13:30:00',)], ['DOB'])\n",
        "df = df.select(to_date(col('DOB'),'dd/MMM/yyyy HH:mm:ss'), to_timestamp(col('DOB'),'dd/MMM/yyyy HH:mm:ss'))\n",
        "df.show()\n",
        "df.printSchema()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------------------------------+---------------------------------------+\n",
            "|to_date(DOB, dd/MMM/yyyy HH:mm:ss)|to_timestamp(DOB, dd/MMM/yyyy HH:mm:ss)|\n",
            "+----------------------------------+---------------------------------------+\n",
            "|                        2019-12-25|                    2019-12-25 13:30:00|\n",
            "+----------------------------------+---------------------------------------+\n",
            "\n",
            "root\n",
            " |-- to_date(DOB, dd/MMM/yyyy HH:mm:ss): date (nullable = true)\n",
            " |-- to_timestamp(DOB, dd/MMM/yyyy HH:mm:ss): timestamp (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dIPQyQV7-Hz5"
      },
      "source": [
        "**What is 3 days earlier that the oldest date and 3 days later than the most recent date?**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PUCEwQkZ-I7h",
        "outputId": "e99ed23d-3a53-45ab-a71a-d5271c4e7610"
      },
      "source": [
        "from pyspark.sql.functions import date_add, date_sub\n",
        "# create a dummy dataframe\n",
        "df = spark.createDataFrame([('1990-01-01',),('1995-01-03',),('2021-03-30',)], ['Date'])\n",
        "# find out the required dates\n",
        "df.select(date_add(max(col('Date')),3), date_sub(min(col('Date')),3)).show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------------------+----------------------+\n",
            "|date_add(max(Date), 3)|date_sub(min(Date), 3)|\n",
            "+----------------------+----------------------+\n",
            "|            2021-04-02|            1989-12-29|\n",
            "+----------------------+----------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7OZElEvcGOD1"
      },
      "source": [
        "<a id='joins-in-pyspark'></a>\n",
        "## Joins in PySpark"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UJBC7r3JFyCL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "be111079-0cc3-4912-858c-0ee535f8e4bd"
      },
      "source": [
        "# Create two dataframes\n",
        "cars_df = spark.createDataFrame([[1, 'Car A'],[2, 'Car B'],[3, 'Car C']], [\"id\", \"car_name\"])\n",
        "car_price_df = spark.createDataFrame([[1, 1000],[2, 2000],[3, 3000]], [\"id\", \"car_price\"])\n",
        "cars_df.show()\n",
        "car_price_df.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+--------+\n",
            "| id|car_name|\n",
            "+---+--------+\n",
            "|  1|   Car A|\n",
            "|  2|   Car B|\n",
            "|  3|   Car C|\n",
            "+---+--------+\n",
            "\n",
            "+---+---------+\n",
            "| id|car_price|\n",
            "+---+---------+\n",
            "|  1|     1000|\n",
            "|  2|     2000|\n",
            "|  3|     3000|\n",
            "+---+---------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U7Py4EYyKJTN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "de7da189-ba03-4aad-862c-cd2bb1ac8724"
      },
      "source": [
        "# Executing an inner join so we can see the id, name and price of each car in one row\n",
        "cars_df.join(car_price_df, cars_df.id == car_price_df.id, 'inner').select(cars_df['id'],cars_df['car_name'],car_price_df['car_price']).show(truncate=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+--------+---------+\n",
            "|id |car_name|car_price|\n",
            "+---+--------+---------+\n",
            "|1  |Car A   |1000     |\n",
            "|3  |Car C   |3000     |\n",
            "|2  |Car B   |2000     |\n",
            "+---+--------+---------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vj0mPaHU5i5n"
      },
      "source": [
        "As you can see, we have done an inner join between two dataframes. The following joins are supported by PySpark:\n",
        "1. inner (default)\n",
        "2. cross\n",
        "3. outer\n",
        "4. full\n",
        "5. full_outer\n",
        "6. left\n",
        "7. left_outer\n",
        "8. right\n",
        "9. right_outer\n",
        "10. left_semi\n",
        "11. left_anti"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HNPhsx8P2tUH"
      },
      "source": [
        "<a id='spark-sql'></a>\n",
        "## Spark SQL"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rHMvBBAh23cw"
      },
      "source": [
        "SQL has been around since the 1970s, and so one can imagine the number of people who made it their bread and butter. As big data came into popularity, the number of professionals with the technical knowledge to deal with it was in shortage. This led to the creation of Spark SQL. To quote the docs:<br>\n",
        ">Spark SQL is a Spark module for structured data processing. Unlike the basic Spark RDD API, the interfaces provided by Spark SQL provide Spark with more information about the structure of both the data and the computation being performed. Internally, Spark SQL uses this extra information to perform extra optimizations.\n",
        "\n",
        "Basically, what you need to know is that Spark SQL is used to execute SQL queries on big data. Spark SQL can also be used to read data from Hive tables and views. Let me explain Spark SQL with an example.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g2DaK9-D7QkX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "19ddfcd8-e777-4d76-c9ed-2b3d69fb8de6"
      },
      "source": [
        "# Load data\n",
        "df = spark.read.csv('cars.csv', header=True, sep=\";\")\n",
        "# Register Temporary Table\n",
        "df.createOrReplaceTempView(\"temp\")\n",
        "# Select all data from temp table\n",
        "spark.sql(\"select * from temp limit 5\").show()\n",
        "# Select count of data in table\n",
        "spark.sql(\"select count(*) as total_count from temp\").show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+----+---------+------------+----------+------+------------+-----+------+\n",
            "|                 Car| MPG|Cylinders|Displacement|Horsepower|Weight|Acceleration|Model|Origin|\n",
            "+--------------------+----+---------+------------+----------+------+------------+-----+------+\n",
            "|Chevrolet Chevell...|18.0|        8|       307.0|     130.0| 3504.|        12.0|   70|    US|\n",
            "|   Buick Skylark 320|15.0|        8|       350.0|     165.0| 3693.|        11.5|   70|    US|\n",
            "|  Plymouth Satellite|18.0|        8|       318.0|     150.0| 3436.|        11.0|   70|    US|\n",
            "|       AMC Rebel SST|16.0|        8|       304.0|     150.0| 3433.|        12.0|   70|    US|\n",
            "|         Ford Torino|17.0|        8|       302.0|     140.0| 3449.|        10.5|   70|    US|\n",
            "+--------------------+----+---------+------------+----------+------+------------+-----+------+\n",
            "\n",
            "+-----------+\n",
            "|total_count|\n",
            "+-----------+\n",
            "|        406|\n",
            "+-----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6i32WE8j_ec8"
      },
      "source": [
        "As you can see, we registered the dataframe as temporary table and then ran basic SQL queries on it. How amazing is that?!<br>\n",
        "If you are a person who is more comfortable with SQL, then this feature is truly a blessing for you! But this raises a question:\n",
        "> *Should I just keep using Spark SQL all the time?*\n",
        "\n",
        "And the answer is, _**it depends**_.<br>\n",
        "So basically, the different functions acts in differnet ways, and depending upon the type of action you are trying to do, the speed at which it completes execution also differs. But as time progress, this feature is getting better and better, so hopefully the difference should be a small margin. There are plenty of analysis done on this, but nothing has a definite answer yet. You can read this [comparative study done by horton works](https://community.cloudera.com/t5/Community-Articles/Spark-RDDs-vs-DataFrames-vs-SparkSQL/ta-p/246547) or the answer to this [stackoverflow question](https://stackoverflow.com/questions/45430816/writing-sql-vs-using-dataframe-apis-in-spark-sql) if you are still curious about it."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x62BiCgBMOtq"
      },
      "source": [
        "<a id='rdd'></a>\n",
        "## RDD"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VGXK6uEuUKRh"
      },
      "source": [
        "> With map, you define a function and then apply it record by record. Flatmap returns a new RDD by first applying a function to all of the elements in RDDs and then flattening the result. Filter, returns a new RDD. Meaning only the elements that satisfy a condition. With reduce, we are taking neighboring elements and producing a single combined result.\n",
        "For example, let's say you have a set of numbers. You can reduce this to its sum by providing a function that takes as input two values and reduces them to one.\n",
        "\n",
        "Some of the reasons you would use a dataframe over RDD are:\n",
        "1.   It's ability to represnt data as rows and columns. But this also means it can only hold structred and semi-structured data.\n",
        "2.   It allows processing data in different formats (AVRO, CSV, JSON, and storage system HDFS, HIVE tables, MySQL).\n",
        "3. It's superior job Optimization capability.\n",
        "4. DataFrame API is very easy to use.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0_WvAgyvR7m6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d8300de7-06e3-4d99-fe2a-f27497e09ff6"
      },
      "source": [
        "cars = spark.sparkContext.textFile('cars.csv')\n",
        "print(cars.first())\n",
        "cars_header = cars.first()\n",
        "cars_rest = cars.filter(lambda line: line!=cars_header)\n",
        "print(cars_rest.first())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Car;MPG;Cylinders;Displacement;Horsepower;Weight;Acceleration;Model;Origin\n",
            "Chevrolet Chevelle Malibu;18.0;8;307.0;130.0;3504.;12.0;70;US\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P65eAFO3Mkdd"
      },
      "source": [
        "**How many cars are there in our csv data?**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vi03EU0CMSmO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "768baf3b-2758-447f-c80f-39fbc1f91f50"
      },
      "source": [
        "cars_rest.map(lambda line: line.split(\";\")).count()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "406"
            ]
          },
          "metadata": {},
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3c4bci70MnlQ"
      },
      "source": [
        "**Display the Car name, MPG, Cylinders, Weight and Origin for the cars Originating in Europe**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fWFpo_WxMnvm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a850cebb-9977-4e12-bb82-5e48d3ffd6b4"
      },
      "source": [
        "# Car name is column  0\n",
        "(cars_rest.filter(lambda line: line.split(\";\")[8]=='Europe').\n",
        " map(lambda line: (line.split(\";\")[0],\n",
        "    line.split(\";\")[1],\n",
        "    line.split(\";\")[2],\n",
        "    line.split(\";\")[5],\n",
        "    line.split(\";\")[8])).collect())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Citroen DS-21 Pallas', '0', '4', '3090.', 'Europe'),\n",
              " ('Volkswagen 1131 Deluxe Sedan', '26.0', '4', '1835.', 'Europe'),\n",
              " ('Peugeot 504', '25.0', '4', '2672.', 'Europe'),\n",
              " ('Audi 100 LS', '24.0', '4', '2430.', 'Europe'),\n",
              " ('Saab 99e', '25.0', '4', '2375.', 'Europe'),\n",
              " ('BMW 2002', '26.0', '4', '2234.', 'Europe'),\n",
              " ('Volkswagen Super Beetle 117', '0', '4', '1978.', 'Europe'),\n",
              " ('Opel 1900', '28.0', '4', '2123.', 'Europe'),\n",
              " ('Peugeot 304', '30.0', '4', '2074.', 'Europe'),\n",
              " ('Fiat 124B', '30.0', '4', '2065.', 'Europe'),\n",
              " ('Volkswagen Model 111', '27.0', '4', '1834.', 'Europe'),\n",
              " ('Volkswagen Type 3', '23.0', '4', '2254.', 'Europe'),\n",
              " ('Volvo 145e (sw)', '18.0', '4', '2933.', 'Europe'),\n",
              " ('Volkswagen 411 (sw)', '22.0', '4', '2511.', 'Europe'),\n",
              " ('Peugeot 504 (sw)', '21.0', '4', '2979.', 'Europe'),\n",
              " ('Renault 12 (sw)', '26.0', '4', '2189.', 'Europe'),\n",
              " ('Volkswagen Super Beetle', '26.0', '4', '1950.', 'Europe'),\n",
              " ('Fiat 124 Sport Coupe', '26.0', '4', '2265.', 'Europe'),\n",
              " ('Fiat 128', '29.0', '4', '1867.', 'Europe'),\n",
              " ('Opel Manta', '24.0', '4', '2158.', 'Europe'),\n",
              " ('Audi 100LS', '20.0', '4', '2582.', 'Europe'),\n",
              " ('Volvo 144ea', '19.0', '4', '2868.', 'Europe'),\n",
              " ('Saab 99le', '24.0', '4', '2660.', 'Europe'),\n",
              " ('Audi Fox', '29.0', '4', '2219.', 'Europe'),\n",
              " ('Volkswagen Dasher', '26.0', '4', '1963.', 'Europe'),\n",
              " ('Opel Manta', '26.0', '4', '2300.', 'Europe'),\n",
              " ('Fiat 128', '24.0', '4', '2108.', 'Europe'),\n",
              " ('Fiat 124 TC', '26.0', '4', '2246.', 'Europe'),\n",
              " ('Fiat x1.9', '31.0', '4', '2000.', 'Europe'),\n",
              " ('Volkswagen Dasher', '25.0', '4', '2223.', 'Europe'),\n",
              " ('Volkswagen Rabbit', '29.0', '4', '1937.', 'Europe'),\n",
              " ('Audi 100LS', '23.0', '4', '2694.', 'Europe'),\n",
              " ('Peugeot 504', '23.0', '4', '2957.', 'Europe'),\n",
              " ('Volvo 244DL', '22.0', '4', '2945.', 'Europe'),\n",
              " ('Saab 99LE', '25.0', '4', '2671.', 'Europe'),\n",
              " ('Fiat 131', '28.0', '4', '2464.', 'Europe'),\n",
              " ('Opel 1900', '25.0', '4', '2220.', 'Europe'),\n",
              " ('Renault 12tl', '27.0', '4', '2202.', 'Europe'),\n",
              " ('Volkswagen Rabbit', '29.0', '4', '1937.', 'Europe'),\n",
              " ('Volkswagen Rabbit', '29.5', '4', '1825.', 'Europe'),\n",
              " ('Volvo 245', '20.0', '4', '3150.', 'Europe'),\n",
              " ('Peugeot 504', '19.0', '4', '3270.', 'Europe'),\n",
              " ('Mercedes-Benz 280s', '16.5', '6', '3820.', 'Europe'),\n",
              " ('Renault 5 GTL', '36.0', '4', '1825.', 'Europe'),\n",
              " ('Volkswagen Rabbit Custom', '29.0', '4', '1940.', 'Europe'),\n",
              " ('Volkswagen Dasher', '30.5', '4', '2190.', 'Europe'),\n",
              " ('BMW 320i', '21.5', '4', '2600.', 'Europe'),\n",
              " ('Volkswagen Rabbit Custom Diesel', '43.1', '4', '1985.', 'Europe'),\n",
              " ('Audi 5000', '20.3', '5', '2830.', 'Europe'),\n",
              " ('Volvo 264gl', '17.0', '6', '3140.', 'Europe'),\n",
              " ('Saab 99gle', '21.6', '4', '2795.', 'Europe'),\n",
              " ('Peugeot 604sl', '16.2', '6', '3410.', 'Europe'),\n",
              " ('Volkswagen Scirocco', '31.5', '4', '1990.', 'Europe'),\n",
              " ('Volkswagen Rabbit Custom', '31.9', '4', '1925.', 'Europe'),\n",
              " ('Mercedes Benz 300d', '25.4', '5', '3530.', 'Europe'),\n",
              " ('Peugeot 504', '27.2', '4', '3190.', 'Europe'),\n",
              " ('Fiat Strada Custom', '37.3', '4', '2130.', 'Europe'),\n",
              " ('Volkswagen Rabbit', '41.5', '4', '2144.', 'Europe'),\n",
              " ('Audi 4000', '34.3', '4', '2188.', 'Europe'),\n",
              " ('Volkswagen Rabbit C (Diesel)', '44.3', '4', '2085.', 'Europe'),\n",
              " ('Volkswagen Dasher (diesel)', '43.4', '4', '2335.', 'Europe'),\n",
              " ('Audi 5000s (diesel)', '36.4', '5', '2950.', 'Europe'),\n",
              " ('Mercedes-Benz 240d', '30.0', '4', '3250.', 'Europe'),\n",
              " ('Renault Lecar Deluxe', '40.9', '4', '1835.', 'Europe'),\n",
              " ('Volkswagen Rabbit', '29.8', '4', '1845.', 'Europe'),\n",
              " ('Triumph TR7 Coupe', '35.0', '4', '2500.', 'Europe'),\n",
              " ('Volkswagen Jetta', '33.0', '4', '2190.', 'Europe'),\n",
              " ('Renault 18i', '34.5', '4', '2320.', 'Europe'),\n",
              " ('Peugeot 505s Turbo Diesel', '28.1', '4', '3230.', 'Europe'),\n",
              " ('Saab 900s', '0', '4', '2800.', 'Europe'),\n",
              " ('Volvo Diesel', '30.7', '6', '3160.', 'Europe'),\n",
              " ('Volkswagen Rabbit l', '36.0', '4', '1980.', 'Europe'),\n",
              " ('Volkswagen Pickup', '44.0', '4', '2130.', 'Europe')]"
            ]
          },
          "metadata": {},
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZYmb5FscMph3"
      },
      "source": [
        "**Display the Car name, MPG, Cylinders, Weight and Origin for the cars Originating in either Europe or Japan**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ZcRIX3mMquF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4f947237-dcfd-4521-de7c-13f796453a3d"
      },
      "source": [
        "# Car name is column  0\n",
        "(cars_rest.filter(lambda line: line.split(\";\")[8] in ['Europe','Japan']).\n",
        " map(lambda line: (line.split(\";\")[0],\n",
        "    line.split(\";\")[1],\n",
        "    line.split(\";\")[2],\n",
        "    line.split(\";\")[5],\n",
        "    line.split(\";\")[8])).collect())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Citroen DS-21 Pallas', '0', '4', '3090.', 'Europe'),\n",
              " ('Toyota Corolla Mark ii', '24.0', '4', '2372.', 'Japan'),\n",
              " ('Datsun PL510', '27.0', '4', '2130.', 'Japan'),\n",
              " ('Volkswagen 1131 Deluxe Sedan', '26.0', '4', '1835.', 'Europe'),\n",
              " ('Peugeot 504', '25.0', '4', '2672.', 'Europe'),\n",
              " ('Audi 100 LS', '24.0', '4', '2430.', 'Europe'),\n",
              " ('Saab 99e', '25.0', '4', '2375.', 'Europe'),\n",
              " ('BMW 2002', '26.0', '4', '2234.', 'Europe'),\n",
              " ('Datsun PL510', '27.0', '4', '2130.', 'Japan'),\n",
              " ('Toyota Corolla', '25.0', '4', '2228.', 'Japan'),\n",
              " ('Volkswagen Super Beetle 117', '0', '4', '1978.', 'Europe'),\n",
              " ('Opel 1900', '28.0', '4', '2123.', 'Europe'),\n",
              " ('Peugeot 304', '30.0', '4', '2074.', 'Europe'),\n",
              " ('Fiat 124B', '30.0', '4', '2065.', 'Europe'),\n",
              " ('Toyota Corolla 1200', '31.0', '4', '1773.', 'Japan'),\n",
              " ('Datsun 1200', '35.0', '4', '1613.', 'Japan'),\n",
              " ('Volkswagen Model 111', '27.0', '4', '1834.', 'Europe'),\n",
              " ('Toyota Corolla Hardtop', '24.0', '4', '2278.', 'Japan'),\n",
              " ('Volkswagen Type 3', '23.0', '4', '2254.', 'Europe'),\n",
              " ('Mazda RX2 Coupe', '19.0', '3', '2330.', 'Japan'),\n",
              " ('Volvo 145e (sw)', '18.0', '4', '2933.', 'Europe'),\n",
              " ('Volkswagen 411 (sw)', '22.0', '4', '2511.', 'Europe'),\n",
              " ('Peugeot 504 (sw)', '21.0', '4', '2979.', 'Europe'),\n",
              " ('Renault 12 (sw)', '26.0', '4', '2189.', 'Europe'),\n",
              " ('Datsun 510 (sw)', '28.0', '4', '2288.', 'Japan'),\n",
              " ('Toyota Corolla Mark II (sw)', '23.0', '4', '2506.', 'Japan'),\n",
              " ('Toyota Corolla 1600 (sw)', '27.0', '4', '2100.', 'Japan'),\n",
              " ('Volkswagen Super Beetle', '26.0', '4', '1950.', 'Europe'),\n",
              " ('Toyota Camry', '20.0', '4', '2279.', 'Japan'),\n",
              " ('Datsun 610', '22.0', '4', '2379.', 'Japan'),\n",
              " ('Mazda RX3', '18.0', '3', '2124.', 'Japan'),\n",
              " ('Fiat 124 Sport Coupe', '26.0', '4', '2265.', 'Europe'),\n",
              " ('Fiat 128', '29.0', '4', '1867.', 'Europe'),\n",
              " ('Opel Manta', '24.0', '4', '2158.', 'Europe'),\n",
              " ('Audi 100LS', '20.0', '4', '2582.', 'Europe'),\n",
              " ('Volvo 144ea', '19.0', '4', '2868.', 'Europe'),\n",
              " ('Saab 99le', '24.0', '4', '2660.', 'Europe'),\n",
              " ('Toyota Mark II', '20.0', '6', '2807.', 'Japan'),\n",
              " ('Datsun B210', '31.0', '4', '1950.', 'Japan'),\n",
              " ('Toyota Corolla 1200', '32.0', '4', '1836.', 'Japan'),\n",
              " ('Audi Fox', '29.0', '4', '2219.', 'Europe'),\n",
              " ('Volkswagen Dasher', '26.0', '4', '1963.', 'Europe'),\n",
              " ('Opel Manta', '26.0', '4', '2300.', 'Europe'),\n",
              " ('Toyota Corolla', '31.0', '4', '1649.', 'Japan'),\n",
              " ('Datsun 710', '32.0', '4', '2003.', 'Japan'),\n",
              " ('Fiat 128', '24.0', '4', '2108.', 'Europe'),\n",
              " ('Fiat 124 TC', '26.0', '4', '2246.', 'Europe'),\n",
              " ('Honda Civic', '24.0', '4', '2489.', 'Japan'),\n",
              " ('Subaru', '26.0', '4', '2391.', 'Japan'),\n",
              " ('Fiat x1.9', '31.0', '4', '2000.', 'Europe'),\n",
              " ('Toyota Corolla', '29.0', '4', '2171.', 'Japan'),\n",
              " ('Toyota Corolla', '24.0', '4', '2702.', 'Japan'),\n",
              " ('Volkswagen Dasher', '25.0', '4', '2223.', 'Europe'),\n",
              " ('Datsun 710', '24.0', '4', '2545.', 'Japan'),\n",
              " ('Volkswagen Rabbit', '29.0', '4', '1937.', 'Europe'),\n",
              " ('Audi 100LS', '23.0', '4', '2694.', 'Europe'),\n",
              " ('Peugeot 504', '23.0', '4', '2957.', 'Europe'),\n",
              " ('Volvo 244DL', '22.0', '4', '2945.', 'Europe'),\n",
              " ('Saab 99LE', '25.0', '4', '2671.', 'Europe'),\n",
              " ('Honda Civic CVCC', '33.0', '4', '1795.', 'Japan'),\n",
              " ('Fiat 131', '28.0', '4', '2464.', 'Europe'),\n",
              " ('Opel 1900', '25.0', '4', '2220.', 'Europe'),\n",
              " ('Renault 12tl', '27.0', '4', '2202.', 'Europe'),\n",
              " ('Volkswagen Rabbit', '29.0', '4', '1937.', 'Europe'),\n",
              " ('Honda Civic', '33.0', '4', '1795.', 'Japan'),\n",
              " ('Volkswagen Rabbit', '29.5', '4', '1825.', 'Europe'),\n",
              " ('Datsun B-210', '32.0', '4', '1990.', 'Japan'),\n",
              " ('Toyota Corolla', '28.0', '4', '2155.', 'Japan'),\n",
              " ('Volvo 245', '20.0', '4', '3150.', 'Europe'),\n",
              " ('Peugeot 504', '19.0', '4', '3270.', 'Europe'),\n",
              " ('Toyota Mark II', '19.0', '6', '2930.', 'Japan'),\n",
              " ('Mercedes-Benz 280s', '16.5', '6', '3820.', 'Europe'),\n",
              " ('Honda Accord CVCC', '31.5', '4', '2045.', 'Japan'),\n",
              " ('Renault 5 GTL', '36.0', '4', '1825.', 'Europe'),\n",
              " ('Datsun F-10 Hatchback', '33.5', '4', '1945.', 'Japan'),\n",
              " ('Volkswagen Rabbit Custom', '29.0', '4', '1940.', 'Europe'),\n",
              " ('Toyota Corolla Liftback', '26.0', '4', '2265.', 'Japan'),\n",
              " ('Subaru DL', '30.0', '4', '1985.', 'Japan'),\n",
              " ('Volkswagen Dasher', '30.5', '4', '2190.', 'Europe'),\n",
              " ('Datsun 810', '22.0', '6', '2815.', 'Japan'),\n",
              " ('BMW 320i', '21.5', '4', '2600.', 'Europe'),\n",
              " ('Mazda RX-4', '21.5', '3', '2720.', 'Japan'),\n",
              " ('Volkswagen Rabbit Custom Diesel', '43.1', '4', '1985.', 'Europe'),\n",
              " ('Mazda GLC Deluxe', '32.8', '4', '1985.', 'Japan'),\n",
              " ('Datsun B210 GX', '39.4', '4', '2070.', 'Japan'),\n",
              " ('Honda Civic CVCC', '36.1', '4', '1800.', 'Japan'),\n",
              " ('Toyota Corolla', '27.5', '4', '2560.', 'Japan'),\n",
              " ('Datsun 510', '27.2', '4', '2300.', 'Japan'),\n",
              " ('Toyota Celica GT Liftback', '21.1', '4', '2515.', 'Japan'),\n",
              " ('Datsun 200-SX', '23.9', '4', '2405.', 'Japan'),\n",
              " ('Audi 5000', '20.3', '5', '2830.', 'Europe'),\n",
              " ('Volvo 264gl', '17.0', '6', '3140.', 'Europe'),\n",
              " ('Saab 99gle', '21.6', '4', '2795.', 'Europe'),\n",
              " ('Peugeot 604sl', '16.2', '6', '3410.', 'Europe'),\n",
              " ('Volkswagen Scirocco', '31.5', '4', '1990.', 'Europe'),\n",
              " ('Honda Accord LX', '29.5', '4', '2135.', 'Japan'),\n",
              " ('Volkswagen Rabbit Custom', '31.9', '4', '1925.', 'Europe'),\n",
              " ('Mazda GLC Deluxe', '34.1', '4', '1975.', 'Japan'),\n",
              " ('Mercedes Benz 300d', '25.4', '5', '3530.', 'Europe'),\n",
              " ('Peugeot 504', '27.2', '4', '3190.', 'Europe'),\n",
              " ('Datsun 210', '31.8', '4', '2020.', 'Japan'),\n",
              " ('Fiat Strada Custom', '37.3', '4', '2130.', 'Europe'),\n",
              " ('Volkswagen Rabbit', '41.5', '4', '2144.', 'Europe'),\n",
              " ('Toyota Corolla Tercel', '38.1', '4', '1968.', 'Japan'),\n",
              " ('Datsun 310', '37.2', '4', '2019.', 'Japan'),\n",
              " ('Audi 4000', '34.3', '4', '2188.', 'Europe'),\n",
              " ('Toyota Corolla Liftback', '29.8', '4', '2711.', 'Japan'),\n",
              " ('Mazda 626', '31.3', '4', '2542.', 'Japan'),\n",
              " ('Datsun 510 Hatchback', '37.0', '4', '2434.', 'Japan'),\n",
              " ('Toyota Corolla', '32.2', '4', '2265.', 'Japan'),\n",
              " ('Mazda GLC', '46.6', '4', '2110.', 'Japan'),\n",
              " ('Datsun 210', '40.8', '4', '2110.', 'Japan'),\n",
              " ('Volkswagen Rabbit C (Diesel)', '44.3', '4', '2085.', 'Europe'),\n",
              " ('Volkswagen Dasher (diesel)', '43.4', '4', '2335.', 'Europe'),\n",
              " ('Audi 5000s (diesel)', '36.4', '5', '2950.', 'Europe'),\n",
              " ('Mercedes-Benz 240d', '30.0', '4', '3250.', 'Europe'),\n",
              " ('Honda Civic 1500 gl', '44.6', '4', '1850.', 'Japan'),\n",
              " ('Renault Lecar Deluxe', '40.9', '4', '1835.', 'Europe'),\n",
              " ('Subaru DL', '33.8', '4', '2145.', 'Japan'),\n",
              " ('Volkswagen Rabbit', '29.8', '4', '1845.', 'Europe'),\n",
              " ('Datsun 280-ZX', '32.7', '6', '2910.', 'Japan'),\n",
              " ('Mazda RX-7 GS', '23.7', '3', '2420.', 'Japan'),\n",
              " ('Triumph TR7 Coupe', '35.0', '4', '2500.', 'Europe'),\n",
              " ('Honda Accord', '32.4', '4', '2290.', 'Japan'),\n",
              " ('Toyota Starlet', '39.1', '4', '1755.', 'Japan'),\n",
              " ('Honda Civic 1300', '35.1', '4', '1760.', 'Japan'),\n",
              " ('Subaru', '32.3', '4', '2065.', 'Japan'),\n",
              " ('Datsun 210 MPG', '37.0', '4', '1975.', 'Japan'),\n",
              " ('Toyota Tercel', '37.7', '4', '2050.', 'Japan'),\n",
              " ('Mazda GLC 4', '34.1', '4', '1985.', 'Japan'),\n",
              " ('Volkswagen Jetta', '33.0', '4', '2190.', 'Europe'),\n",
              " ('Renault 18i', '34.5', '4', '2320.', 'Europe'),\n",
              " ('Honda Prelude', '33.7', '4', '2210.', 'Japan'),\n",
              " ('Toyota Corolla', '32.4', '4', '2350.', 'Japan'),\n",
              " ('Datsun 200SX', '32.9', '4', '2615.', 'Japan'),\n",
              " ('Mazda 626', '31.6', '4', '2635.', 'Japan'),\n",
              " ('Peugeot 505s Turbo Diesel', '28.1', '4', '3230.', 'Europe'),\n",
              " ('Saab 900s', '0', '4', '2800.', 'Europe'),\n",
              " ('Volvo Diesel', '30.7', '6', '3160.', 'Europe'),\n",
              " ('Toyota Cressida', '25.4', '6', '2900.', 'Japan'),\n",
              " ('Datsun 810 Maxima', '24.2', '6', '2930.', 'Japan'),\n",
              " ('Volkswagen Rabbit l', '36.0', '4', '1980.', 'Europe'),\n",
              " ('Mazda GLC Custom l', '37.0', '4', '2025.', 'Japan'),\n",
              " ('Mazda GLC Custom', '31.0', '4', '1970.', 'Japan'),\n",
              " ('Nissan Stanza XE', '36.0', '4', '2160.', 'Japan'),\n",
              " ('Honda Accord', '36.0', '4', '2205.', 'Japan'),\n",
              " ('Toyota Corolla', '34.0', '4', '2245', 'Japan'),\n",
              " ('Honda Civic', '38.0', '4', '1965.', 'Japan'),\n",
              " ('Honda Civic (auto)', '32.0', '4', '1965.', 'Japan'),\n",
              " ('Datsun 310 GX', '38.0', '4', '1995.', 'Japan'),\n",
              " ('Toyota Celica GT', '32.0', '4', '2665.', 'Japan'),\n",
              " ('Volkswagen Pickup', '44.0', '4', '2130.', 'Europe')]"
            ]
          },
          "metadata": {},
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3wn2zXe7TbI3"
      },
      "source": [
        "<a id='user-defined-functions-udf'></a>\n",
        "## User-Defined Functions (UDF)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w0YWspcTRrin"
      },
      "source": [
        "PySpark User-Defined Functions (UDFs) help you convert your python code into a scalable version of itself. It comes in handy more than you can imagine, but beware, as the performance is less when you compare it with pyspark functions. You can view examples of how UDF works [here](https://docs.databricks.com/spark/latest/spark-sql/udf-python.html). What I will give in this section is some theory on how it works, and why it is slower.\n",
        "\n",
        "When you try to run a UDF in PySpark, each executor creates a python process. Data will be serialised and deserialised between each executor and python. This leads to lots of performance impact and overhead on spark jobs, making it less efficent than using spark dataframes. Apart from this, sometimes you might have memory issues while using UDFs. The Python worker consumes huge off-heap memory and so it often leads to memoryOverhead, thereby failing your job. Keeping these in mind, I wouldn't recommend using them, but at the end of the day, your choice."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yv7ODDTQRwVt"
      },
      "source": [
        "<a id='common-questions'></a>\n",
        "# Common Questions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6z9gkoE2R1m1"
      },
      "source": [
        "<a id='recommended-ide'></a>\n",
        "## Recommended IDE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BpMTbRggR5Z8"
      },
      "source": [
        "I personally prefer [PyCharm](https://www.jetbrains.com/pycharm/) while coding in Python/PySpark. It's based on IntelliJ IDEA so it has a lot of features! And the main advantage I have felt is the ease of installing PySpark and other packages. You can customize it with themes and plugins, and it lets you enhance productivity while coding by providing some features like suggestions, local VCS etc."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VZ1bYvF8R8Dc"
      },
      "source": [
        "<a id='submitting-a-spark-job'></a>\n",
        "## Submitting a Spark Job"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3EQWnq23SCbE"
      },
      "source": [
        "The python syntax for running jobs is: `python <file_name>.py <arg1> <arg2> ...`\n",
        "<br>But when you submit a spark job you have to use spark-submit to run the application.\n",
        "\n",
        "Here is a simple example of a spark-submit command:\n",
        "`spark-submit filename.py --named_argument 'arguemnt value'`<br>\n",
        "Here, named_argument is an argument that you are reading from inside your script.\n",
        "\n",
        "There are other options you can pass in the command, like:<br>\n",
        "`--py-files` which helps you pass a python file to read in your file,<br>\n",
        "`--files` which helps pass other files like txt or config,<br>\n",
        "`--deploy-mode` which tells wether to deploy your worker node on cluster or locally <br>\n",
        "`--conf` which helps pass different configurations, like memoryOverhead, dynamicAllocation etc.\n",
        "\n",
        "There is an [entire page](https://spark.apache.org/docs/latest/submitting-applications.html) in spark documentation dedicated to this. I highly recommend you go through it once."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oVwGYAZZiyGV"
      },
      "source": [
        "<a id='creating-dataframes'></a>\n",
        "## Creating Dataframes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TvndhPjoi0er"
      },
      "source": [
        "When getting started with dataframes, the most common question is: *'How do I create a dataframe?'* <br>\n",
        "Below, you can see how to create three kinds of dataframes:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QXmRD3hHlM-f"
      },
      "source": [
        "### Create a totally empty dataframe"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ktkb6s-kjtgG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "535e5d92-1fb5-43bb-8c82-49214b39731f"
      },
      "source": [
        "from pyspark.sql.types import StructType\n",
        "sc = spark.sparkContext\n",
        "#Create empty df\n",
        "schema = StructType([])\n",
        "empty = spark.createDataFrame(sc.emptyRDD(), schema)\n",
        "empty.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "++\n",
            "||\n",
            "++\n",
            "++\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mg5K3nz_lSDe"
      },
      "source": [
        "### Create an empty dataframe with header"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9raf4CkRjuTr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a0df3fe9-7f0a-488c-d39f-612aa87dbdb2"
      },
      "source": [
        "from pyspark.sql.types import StructType, StructField\n",
        "#Create empty df with header\n",
        "schema_header = StructType([StructField(\"name\", StringType(), True)])\n",
        "empty_with_header = spark.createDataFrame(sc.emptyRDD(), schema_header)\n",
        "empty_with_header.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+\n",
            "|name|\n",
            "+----+\n",
            "+----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y1ZNOx7ilUnd"
      },
      "source": [
        "### Create a dataframe with header and data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TvzyL46QkJBl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3e6fc943-dafe-4de7-ede2-de29764afa3a"
      },
      "source": [
        "from pyspark.sql import Row\n",
        "mylist = [\n",
        "  {\"name\":'Alice',\"age\":13},\n",
        "  {\"name\":'Jacob',\"age\":24},\n",
        "  {\"name\":'Betty',\"age\":135},\n",
        "]\n",
        "spark.createDataFrame(Row(**x) for x in mylist).show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+---+\n",
            "| name|age|\n",
            "+-----+---+\n",
            "|Alice| 13|\n",
            "|Jacob| 24|\n",
            "|Betty|135|\n",
            "+-----+---+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VnRMckA5nLoJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "15590345-f6a7-4a93-d1ed-b13d2a923d1f"
      },
      "source": [
        "# You can achieve the same using this - note that we are using spark context here, not a spark session\n",
        "from pyspark.sql import Row\n",
        "df = sc.parallelize([\n",
        "        Row(name='Alice', age=13),\n",
        "        Row(name='Jacob', age=24),\n",
        "        Row(name='Betty', age=135)]).toDF()\n",
        "df.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+---+\n",
            "| name|age|\n",
            "+-----+---+\n",
            "|Alice| 13|\n",
            "|Jacob| 24|\n",
            "|Betty|135|\n",
            "+-----+---+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f3crkAQVlxKp"
      },
      "source": [
        "<a id='drop-duplicates'></a>\n",
        "## Drop Duplicates"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4IHrYEwHmBcc"
      },
      "source": [
        "As mentioned earlier, there are two easy to remove duplicates from a dataframe. We have already seen the usage of distinct under <a href=\"#get-distinct-rows\">Get Distinct Rows</a>  section.\n",
        "I will expalin how to use the `dropDuplicates()` function to achieve the same.\n",
        "\n",
        "> `drop_duplicates()` is an alias for `dropDuplicates()`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wOuHRAPJmWen",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "03e2e85d-4d4d-4dc1-9d5f-d88c7413309e"
      },
      "source": [
        "from pyspark.sql import Row\n",
        "from pyspark.sql import Row\n",
        "mylist = [\n",
        "  {\"name\":'Alice',\"age\":5,\"height\":80},\n",
        "  {\"name\":'Jacob',\"age\":24,\"height\":80},\n",
        "  {\"name\":'Alice',\"age\":5,\"height\":80}\n",
        "]\n",
        "df = spark.createDataFrame(Row(**x) for x in mylist)\n",
        "df.dropDuplicates().show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+---+------+\n",
            "| name|age|height|\n",
            "+-----+---+------+\n",
            "|Alice|  5|    80|\n",
            "|Jacob| 24|    80|\n",
            "+-----+---+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zMv7A-2Hnmjh"
      },
      "source": [
        "`dropDuplicates()` can also take in an optional parameter called *subset* which helps specify the columns on which the duplicate check needs to be done on."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SHnFylV1n8to",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9b7c3c0d-1729-4f6d-c803-6047558ba2da"
      },
      "source": [
        "df.dropDuplicates(subset=['height']).show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+---+------+\n",
            "| name|age|height|\n",
            "+-----+---+------+\n",
            "|Alice|  5|    80|\n",
            "+-----+---+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bAS4DKxjqI7H"
      },
      "source": [
        "<a id='fine-tuning-a-pyspark-job'></a>\n",
        "## Fine Tuning a Spark Job"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2d9pLlDl76dM"
      },
      "source": [
        "Before we begin, please note that this entire section is written purely based on experience. It might differ with use cases, but it will help you get a better understanding of what you should be looking for, or act as a guidance to achieve your aim.\n",
        "\n",
        ">Spark Performance Tuning refers to the process of adjusting settings to record for memory, cores, and instances used by the system. This process guarantees that the Spark has a flawless performance and also prevents bottlenecking of resources in Spark.\n",
        "\n",
        "Considering you are using Amazon EMR to execute your spark jobs, there are three aspects you need to take care of:\n",
        "1. EMR Sizing\n",
        "2. Spark Configurations\n",
        "3. Job Tuning\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vIbXZT29JxmG"
      },
      "source": [
        "<a id='emr-sizing'></a>\n",
        "### EMR Sizing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Rv2SM_-KA8W"
      },
      "source": [
        "Sizing your EMR is extremely important, as this affects the efficency of your spark jobs. Apart from the cost factor, the maximum number of nodes and memory your job can use will be decided by this. If you spin up a EMR with high specifications, that obviously means you are paying more for it, so we should ideally utilize it to the max. These are the guidelines that I follow to make sure the EMR is rightly sized:\n",
        "\n",
        "1. Size of the input data (include all the input data) on the disk.\n",
        "2. Whether the jobs have transformations or just a straight pass through.<br> Assess the joins and the complex joins involved.\n",
        "3. Size of the output data on the disk.\n",
        "\n",
        "Look at the above criteria against the memory you need to process, and the disk space you would need. Start with a small configuration, and keep adding nodes to arrive at an optimal configuration. In case you are wondering about the *Execution time vs EMR configuration* factor, please understand that it is okay for a job to run longer, rather than adding more resources to the cluster. For example, it is okay to run a job for 40 mins job on a 5 node cluster, rather than running a job in 10 mins on a 15 node cluster.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pmeFEgv6QTST"
      },
      "source": [
        "Another thing you need to know about EMRs, are the different kinds of EC2 instance types provided by Amazon. I will briefly talk about them, but I strongly recommend you to read more about it from the [official documentation](https://aws.amazon.com/ec2/instance-types/). There are 5 types of instance classes. Based on the job you want to run, you can decide which one to use:\n",
        "\n",
        ">Instance Class | Description\n",
        ">--- | ---\n",
        ">General purpose | Balance of compute, memory and networking resources\n",
        ">Compute optimized | Ideal for compute bound applications that benefit from high performance processors\n",
        ">Memory optimized | Designed to deliver fast performance for workloads that process large data sets in memory\n",
        ">Storage optimized | For workloads that require high, sequential read and write access to very large data sets on local storage\n",
        ">GPU instances | Use hardware accelerators, or co-processors, to perform high demanding functions, more efficiently than is possible in software running on CPUs\n",
        "\n",
        "The configuration (memory, storage, cpu, network performance) will differ based on the instance class you choose.<br>\n",
        "To help make life easier, here is what I do when I get into a predicament about which one to go with: <br>\n",
        " 1. Visit [ec2instances](https://www.ec2instances.info/)\n",
        " 2. Choose the EC2 instances in question\n",
        " 3. Click on compare selected\n",
        "\n",
        "This will easily help you undesrstand what you are getting into, and thereby help you make the best choice! The site was built by [Garret Heaton](https://github.com/powdahound)(founder of Swoot), and has helped me countless number of times to make an informed decision."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "snACMwZug5Yn"
      },
      "source": [
        "<a id='spark-configurations'></a>\n",
        "### Spark Configurations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uFJNpK06hnpo"
      },
      "source": [
        "There are a ton of [configurations](https://spark.apache.org/docs/latest/configuration.html) that you can tweak when it comes to Spark. Here, I will be noting down some of the configurations which I use, which have worked well for me. Alright! let's get into it!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dABRu9eokZxw"
      },
      "source": [
        "#### Job Scheduling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8fRFs6atkdxS"
      },
      "source": [
        "When you submit your job in a cluster, it will be given to Spark Schedulers, which is responsible for materializing a logical plan for your job. There are two types of [job scheduling](https://spark.apache.org/docs/latest/job-scheduling.html):\n",
        "1. FIFO<br>\n",
        "By default, Spark’s scheduler runs jobs in FIFO fashion. Each job is divided into stages (e.g. map and reduce phases), and the first job gets priority on all available resources while its stages have tasks to launch, then the second job gets priority, etc. If the jobs at the head of the queue don’t need to use the whole cluster, later jobs can start to run right away, but if the jobs at the head of the queue are large, then later jobs may be delayed significantly.\n",
        "2. FAIR<br>\n",
        "The fair scheduler supports grouping jobs into pools and setting different scheduling options (e.g. weight) for each pool. This can be useful to create a high-priority pool for more important jobs, for example, or to group the jobs of each user together and give users equal shares regardless of how many concurrent jobs they have instead of giving jobs equal shares. This approach is modeled after the Hadoop Fair Scheduler.\n",
        "\n",
        "> I personally prefer using the FAIR mode, and this can be set by adding `.config(\"spark.scheduler.mode\", \"FAIR\")` when you create your SparkSession.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xrgbAiZHnq_U"
      },
      "source": [
        "#### Serializer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N7ZYGGcYsRzB"
      },
      "source": [
        "We have two types of [serializers](https://spark.apache.org/docs/latest/tuning.html#data-serialization) available:\n",
        "1. Java serialization\n",
        "2. Kryo serialization\n",
        "\n",
        "Kryo is significantly faster and more compact than Java serialization (often as much as 10x), but does not support all Serializable types and requires you to register the classes you’ll use in the program in advance for best performance.\n",
        "\n",
        "Java serialization is used by default because if you have custom class that extends Serializable it can be easily used. You can also control the performance of your serialization more closely by extending java.io.Externalizable\n",
        "\n",
        "> The general recommendation is to use Kyro as the serializer whenver possible, as it leads to much smaller sizes than Java serialization. It can be added by using `.config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\")` when you create your SparkSession.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "244El832wT8f"
      },
      "source": [
        "#### Shuffle Behaviour"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XWsBFnA8xpeF"
      },
      "source": [
        "It is generally a good idea to compress the output file after the map phase. The `spark.shuffle.compress` property decides whether to do the compression or not. The compression used is `spark.io.compression.codec`.\n",
        "\n",
        "> The property can be added by using `.config(\"spark.shuffle.compress\", \"true\")` when you create your SparkSession."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H5cFCvbczHz_"
      },
      "source": [
        "#### Compression and Serialization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LHU5lfyFzKw9"
      },
      "source": [
        "There are 4 defaiult codecs spark provides to compress internal data such as RDD partitions, event log, broadcast variables and shuffle outputs. They are:\n",
        "\n",
        "1. lz4\n",
        "2. lzf\n",
        "3. snappy\n",
        "4. zstd\n",
        "\n",
        "> The decision on which to use rests upon the use case. I generally use the `snappy` compression. Google created Snappy because they needed something that offered very fast compression at the expense of final size. Snappy is fast, stable and free, but it increases the size more than the other codecs. At the same time, since compute costs will be less, it seems like balanced trade off. The property can be added by using `.config(\"spark.io.compression.codec\", \"snappy\")` when you create your SparkSession.\n",
        "\n",
        "This [session](https://databricks.com/session/best-practice-of-compression-decompression-codes-in-apache-spark) explains the best practice of compression/decompression codes in Apache Spark. I recommend you to take a look at it before taking a decision."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rdGARl-D3n-l"
      },
      "source": [
        "#### Scheduling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cm0ExAoS4RU6"
      },
      "source": [
        "The property `spark.speculation` performs speculative execution of tasks. This means if one or more tasks are running slowly in a stage, they will be re-launched. Speculative execution will not stop the slow running task but it launches the new task in parallel.\n",
        "\n",
        "> I usually disable this option by adding `.config(\"spark.speculation\", \"false\") ` when I create the SparkSession."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aaxfGqYZ6Iqz"
      },
      "source": [
        "#### Application Properties"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ERovEKOU6TNE"
      },
      "source": [
        "There are mainly two application properties that you should know about:\n",
        "\n",
        "1. spark.driver.memoryOverhead - The amount of off-heap memory to be allocated per driver in cluster mode, in MiB unless otherwise specified. This is memory that accounts for things like VM overheads, interned strings, other native overheads, etc. This tends to grow with the container size (typically 6-10%). This option is currently supported on YARN and Kubernetes.\n",
        "\n",
        "2. spark.executor.memoryOverhead - The amount of off-heap memory to be allocated per executor, in MiB unless otherwise specified. This is memory that accounts for things like VM overheads, interned strings, other native overheads, etc. This tends to grow with the executor size (typically 6-10%). This option is currently supported on YARN and Kubernetes.\n",
        "\n",
        "> If you ever face an issue like `Container killed by YARN for exceeding memory limits`, know that it is because you have not specified enough memory Overhead for your job to successfully execute. The default value for Overhead is 10% of avaialbe memory (driver/executor sepearte), with minimum of 384.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BG09dDdL6Tvt"
      },
      "source": [
        "#### Dynamic Allocation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m_lb-JI78CVT"
      },
      "source": [
        "И наконец, я хочу рассказать о динамическом распределении. Это свойство я постоянно использую при выполнении заданий. Это свойство по умолчанию имеет значение False. Как следует из названия, оно устанавливает, использовать ли динамическое распределение ресурсов, которое увеличивает или уменьшает количество исполнителей, зарегистрированных в этом приложении, в зависимости от рабочей нагрузки. Поистине замечательная функция, и самое большое преимущество ее использования заключается в том, что она поможет наилучшим образом использовать все имеющиеся у вас ресурсы! Недостатком этой функции является то, что она не очень хорошо проявляет себя при параллельном выполнении задач. Поскольку большая часть ресурсов будет использована первой задачей, второй придется ждать, пока освободится какой-то ресурс. В то же время, если оба задания будут поданы в одно и то же время, ресурсы будут разделены между ними, хотя и не поровну. Кроме того, не гарантируется *всегда* использование наиболее оптимальных конфигураций. Но во всех моих тестах результаты были отличными!\n",
        "\n",
        "> Если вы планируете использовать эту функцию, вы можете передать необходимые конфигурации с помощью команды spark-submit. Вам необходимо помнить о четырех конфигурациях:<br>\n",
        "```\n",
        "--conf spark.dynamicAllocation.enabled=true\n",
        "--conf spark.dynamicAllocation.initialExecutors\n",
        "--conf spark.dynamicAllocation.minExecutors\n",
        "--conf spark.dynamicAllocation.maxExecutors\n",
        "```\n",
        "\n",
        "You can read more about this feature [here](https://spark.apache.org/docs/latest/configuration.html#dynamic-allocation) and [here](https://stackoverflow.com/questions/40200389/how-to-execute-spark-programs-with-dynamic-resource-allocation).\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CJlmPbLYKKFA"
      },
      "source": [
        "<a id='job-tuning'></a>\n",
        "### Job Tuning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vccmILvVWewW"
      },
      "source": [
        "Помимо настройки EMR и Spark, существует еще один способ оптимизации - это настройка самого задания для получения эффективных результатов. Я расскажу о некоторых таких техниках, которые помогут вам достичь этой цели. The [Spark Programming Guide](https://spark.apache.org/docs/2.1.1/programming-guide.html) подробно рассказывает об этих концепциях. Если вы предпочитаете смотреть видео, а не читать, я настоятельно рекомендую [A Deep Dive into Proper Optimization for Spark Jobs](https://youtu.be/daXEp4HmS-E) Дэниела Томса из Databricks, которая показалась мне очень полезной и информативной!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r-R5ijHrKSg0"
      },
      "source": [
        "#### Broadcast Joins (Broadcast Hash Join)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dvO0z5EpM5U8"
      },
      "source": [
        "Для некоторых заданий эффективность можно повысить, если кэшировать их в памяти. Broadcast Hash Join(BHJ) - это такая техника, которая поможет вам оптимизировать запросы на соединение, когда размер одной из сторон данных мал. >BroadCast joins являются самыми быстрыми, но их недостатком является то, что они потребляют больше памяти как на исполнителе, так и на драйвере.\n",
        "\n",
        "Следующие шаги дают представление о том, как это работает, что поможет вам понять, в каких случаях это может быть использовано:<br>\n",
        "1. Входной файл (меньшая из двух таблиц), подлежащий трансляции, считывается исполнителями параллельно в свою рабочую память.\n",
        "2. Все данные от исполнителей собираются в драйвер (отсюда необходимость в большей памяти драйвера).\n",
        "3. Затем драйвер транслирует объединенный набор данных (полную копию) каждому исполнителю.\n",
        "4. Размер транслируемого набора данных может быть в несколько (10-20+) раз больше, чем входной в памяти, из-за таких факторов, как десериализация.\n",
        "5. Исполнители будут хранить в памяти те части, которые они прочитали первыми, а также полную копию, что приведет к большим потребностям в памяти.\n",
        "\n",
        "Некоторые вещи, которые следует помнить о BHJ:\n",
        "1. Рекомендуется использовать широковещательные соединения только в небольших наборах данных (например, в таблице dimesnion).\n",
        "2. Spark не гарантирует, что BHJ будет выбран всегда, поскольку не все случаи (например, полное внешнее соединение) поддерживают BHJ.\n",
        "3. Вы можете заметить перекосы в задачах из-за неравномерного размера разделов; особенно при агрегировании, объединении и т. д. Это можно выровнять, введя значение Salt (случайная величина).<br>* Предлагаемая формула для значения salt:* random(0 - (shuffle partition count - 1))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UOvEVcieVn7e"
      },
      "source": [
        "#### Spark Partitions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ocpQiqOtVqPz"
      },
      "source": [
        "Раздел в Spark - это атомарный кусок данных (логическое разделение данных), хранящийся на узле в кластере. Разделы - это основные единицы параллелизма в Spark. Слишком большое или слишком малое количество разделов - не идеальное решение. Количество разделов в Spark должно определяться исходя из конфигурации кластера и требований приложения. Увеличение числа разделов приведет к тому, что каждый раздел будет содержать меньше данных или не будет содержать их вовсе. В общем случае разделение в spark можно разделить на три части:\n",
        "1. Ввод\n",
        "2. Перемешивание\n",
        "3. Выход\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2KY1mxZVfNsl"
      },
      "source": [
        "##### Input"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dKw48h9eEbPa"
      },
      "source": [
        "Обычно Spark хорошо справляется с определением идеальной конфигурации, за исключением очень специфических случаев. Рекомендуется использовать конфигурацию spark по умолчанию, если только:\n",
        "1. Увеличение параллелизма\n",
        "2. Сильно вложенные данные\n",
        "3. Генерация данных (explode)\n",
        "4. Источник не является оптимальным\n",
        "5. Вы используете UDF-файлы\n",
        "\n",
        "`spark.sql.files.maxpartitionBytes`: Это свойство указывает максимальное количество байт, которое должно быть упаковано в один раздел при чтении файлов (по умолчанию 128 МБ). Используйте это свойство для увеличения параллелизма при чтении входных данных. Например, если у вас больше ядер, то вы можете увеличить количество параллельных задач, что обеспечит использование всех ядер кластера и увеличит скорость выполнения задачи."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qwEBu3T3EbfD"
      },
      "source": [
        "##### Shuffle"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wx0iQUpFEbus"
      },
      "source": [
        "Одной из основных причин снижения производительности большинства заданий является неправильная установка количества разделов для перетасовки. По умолчанию это значение установлено на 200. Почти во всех ситуациях это не является идеальным. Если вы имеете дело с сателлитами размером менее 20 ГБ, 200 вполне подойдет, но в противном случае значение нужно изменить. Для большинства случаев можно использовать следующее уравнение, чтобы найти правильное значение:\n",
        ">`Partition Count = Stage Input Data / Target Size` где <br>\n",
        "`Largest Shuffle Stage (Target Size) < 200MB/partition` в большинстве случаев.<br>\n",
        "`spark.sql.shuffle.partitions` Свойство используется для установки идеального значения количества разделов.\n",
        "\n",
        "Если вы когда-нибудь заметите, что целевой размер находится в диапазоне ТБ, значит, что-то не так, и вам следует изменить его на 200 или пересчитать. Перемешивание разделов можно настроить для каждого действия (не преобразования) в сценарии spark.\n",
        "\n",
        "Давайте на примере объясним этот сценарий: <br> Предположим, что вход этапа shuffle = 210 ГБ. <br> Partition Count = Stage Input Data / Target Size = 210000 MB/200 MB = 1050. <br> Как видите, разделов для шаффла должно быть 1050, а не 200.\n",
        "\n",
        "Но если в вашем кластере 2000 ядер, то установите разделы шаффла на 2000. >В большом кластере, работающем с большими данными, никогда не устанавливайте разделы шаффла меньше, чем общее количество ядер.\n",
        "\n",
        "\n",
        "\n",
        "Этапы перетасовки почти всегда предшествуют этапам записи, и большое количество разделов перетасовки приводит к появлению маленьких файлов на выходе. Чтобы решить эту проблему, используйте localCheckPoint непосредственно перед записью и выполняйте вызов coalesce. Этот localCheckPoint записывает Shuffle Partition на локальный диск исполнителя, а затем коалесцирует в меньшее количество разделов и, следовательно, улучшает общую производительность как этапа shuffle, так и этапа записи."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-HxUYv77EdSv"
      },
      "source": [
        "##### Output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CPA6YRYrEdgG"
      },
      "source": [
        "Существуют различные методы записи данных. При записи данных можно контролировать размер, состав, количество файлов на выходе и даже количество записей в каждом файле. При записи данных можно увеличить параллелизм, тем самым обеспечив использование всех имеющихся ресурсов. Но такой подход приведет к увеличению количества мелких файлов. Обычно это не проблема, но если вы хотите получить файлы большего размера, вам придется использовать одну из техник уплотнения, желательно в кластере с меньшей конфигурацией. Существует множество способов изменить состав выходных данных. Помните об этих двух способах изменения состава:\n",
        "1. Объединение: Используйте эту функцию, чтобы уменьшить количество разделов.\n",
        "2. Переразбиение: Используется очень редко и никогда для уменьшения количества разделов<br> a. Range Paritioner - Разбивает данные либо на основе некоторого отсортированного порядка ИЛИ набора отсортированных диапазонов ключей. <br> b. Hash-разделитель - распределяет данные в разделе на основе значения ключа. Хеш-разбиение может сделать распределенные данные перекошенными."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NZ5xsdWmNOVz"
      },
      "source": [
        "<a id='best-practices'></a>\n",
        "### Best Practices"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NUKLZ8G8NVuR"
      },
      "source": [
        "Попробуйте включить их в свои привычки кодирования для повышения производительности:\n",
        "1.   Не используйте NOT IN, используйте NOT EXISTS.\n",
        "2.   Удаляйте графы, отличительные графы (используйте approxCountDIstinct).\n",
        "3.   Заранее отбрасывайте дубликаты.\n",
        "4.   Всегда отдавайте предпочтение SQL-функциям, а не PandasUDF.\n",
        "5.   Эффективно используйте разделы Hive.\n",
        "6.   Эффективно используйте пользовательский интерфейс Spark.\n",
        "7.   Избегайте переливов Shuffle.\n",
        "8.   Стремитесь к целевой загрузке кластера не менее чем на 70 %.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FdDkc0SHZP2-",
        "outputId": "1d2579de-c15c-4363-a5ec-f3ade450b728"
      },
      "source": [
        "%%shell\n",
        "jupyter nbconvert --to html /content/Colab_и_PySpark.ipynb"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[NbConvertApp] Converting notebook /content/Colab_и_PySpark.ipynb to html\n",
            "[NbConvertApp] Writing 892645 bytes to /content/Colab_и_PySpark.html\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 73
        }
      ]
    }
  ]
}